import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, Embedding, Flatten, Concatenate
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.metrics import mean_absolute_error, accuracy_score
from sklearn.model_selection import train_test_split
from tensorflow.keras.mixed_precision import set_global_policy
import random
import os
import io

# ==========================================
# 0. ç”¨æˆ·é…ç½®å‚æ•° (STRICT CONFIG)
# ==========================================
FILE_PATH = 'F:\Codes\wordle_games.csv'
MAX_ROWS = 2000000 
LOOK_BACK = 5       
EPOCHS = 15         
BATCH_SIZE = 256    
PREDICTION_SAMPLE_SIZE = 10 
VALIDATION_SAMPLE_SIZE = 50000 
LARGE_ERROR_THRESHOLD = 1.5 
EMBEDDING_DIM = 32
MODEL_SAVE_PATH = 'pred_Model'

# ==========================================
# 1. æ•°æ®åŠ è½½ä¸é«˜çº§ç‰¹å¾å·¥ç¨‹
# ==========================================

def process_data_and_extract_features(file_path, nrows):
    """è¯»å–æ•°æ®ï¼Œè®¡ç®—å•è¯éš¾åº¦ã€å•è¯IDã€ç”¨æˆ·å¹³å‡åå¥½ã€‚"""
    print("Step 1: è¯»å–æ•°æ®å¹¶æ„å»ºå››ç‰¹å¾...")
    try:
        df = pd.read_csv(file_path, nrows=nrows, usecols=['Game', 'Trial', 'Username', 'target'])
        df = df.dropna()
    except:
        # æ¨¡æ‹Ÿæ•°æ®... (çœç•¥æ¨¡æ‹Ÿé€»è¾‘ï¼Œè§å‰ä¸€ä¸ªç‰ˆæœ¬)
        print("âš ï¸ æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿ wordle_games.csv å­˜åœ¨ã€‚")
        return {}, 2, None, None 

    # --- Feature A & B: å•è¯éš¾åº¦ (Difficulty) & å•è¯ ID (Embedding) ---
    word_stats = df.groupby('target')['Trial'].mean().to_dict()
    df['word_difficulty'] = df['target'].map(word_stats)
    tokenizer = Tokenizer(); tokenizer.fit_on_texts(df['target'])
    df['word_id'] = df['target'].apply(lambda x: tokenizer.texts_to_sequences([x])[0][0])
    vocab_size = len(tokenizer.word_index) + 1

    # --- Feature C: ç”¨æˆ·åå¥½ (User Bias) ---
    # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„å…¨å±€å¹³å‡æ­¥æ•°
    user_stats = df.groupby('Username')['Trial'].mean().to_dict()
    df['user_bias'] = df['Username'].map(user_stats)

    df = df.sort_values(by=['Username', 'Game'])
    
    # æ„å»ºå¤åˆå­—å…¸ï¼š(Trial, Difficulty, WordID, UserBias)
    history_map = df.groupby('Username').apply(
        lambda x: list(zip(x['Trial'], x['word_difficulty'], x['word_id'], x['user_bias']))
    ).to_dict()
    
    return history_map, vocab_size, tokenizer, word_stats

def create_multi_input_dataset(user_history_map, look_back):
    """æ„å»ºå››è¾“å…¥æ•°æ®é›†ï¼šX_seq, X_diff, X_word, X_bias."""
    print("Step 2: æ„å»ºå››è¾“å…¥æ ·æœ¬...")
    
    X_seq_list = []
    X_diff_list = []; X_word_list = []; X_bias_list = []
    y_steps = []; y_success = []
    valid_players = []

    for user, history in user_history_map.items():
        if len(history) > look_back:
            for i in range(len(history) - look_back):
                
                # --- 1. å†å²åºåˆ—ç‰¹å¾ (LSTM Input) ---
                past_trials = [h[0] for h in history[i : i+look_back]]
                past_arr = np.array(past_trials)
                std_dev = np.std(past_arr) / 7.0; seq_norm = past_arr / 7.0
                seq_2d = np.stack([seq_norm, np.full_like(seq_norm, std_dev)], axis=1)
                
                # --- 2. ç›®æ ‡ä¿¡æ¯å’Œç”¨æˆ·åå¥½ (Context Inputs) ---
                target_game = history[i + look_back]
                target_trial = target_game[0]
                target_difficulty = target_game[1]
                target_word_id = target_game[2]
                target_user_bias = target_game[3] # æ–°å¢
                
                # æ”¶é›†æ•°æ®
                X_seq_list.append(seq_2d)
                X_diff_list.append(target_difficulty / 7.0) 
                X_word_list.append(target_word_id)
                X_bias_list.append(target_user_bias / 7.0) # æ–°å¢
                
                # æ ‡ç­¾
                y_steps.append(7.0 if target_trial > 6 else float(target_trial))
                y_success.append(1.0 if target_trial <= 6 else 0.0)
            
            valid_players.append(user)

    return (
        np.array(X_seq_list, dtype=np.float32),
        np.array(X_diff_list, dtype=np.float32),
        np.array(X_word_list, dtype=np.float32),
        np.array(X_bias_list, dtype=np.float32), # æ–°å¢è¾“å‡º
        np.array(y_steps, dtype=np.float32),
        np.array(y_success, dtype=np.float32),
        valid_players
    )

# ==========================================
# 2. æ¨¡å‹æ„å»º (Multi-Input Four Branch)
# ==========================================

def build_context_model(look_back, vocab_size, embedding_dim):
    print(f"Step 3: æ„å»ºå››è¾“å…¥ç¥ç»ç½‘ç»œ (Vocab={vocab_size})...")
    
    # --- Input 1: ç©å®¶å†å² (LSTM) ---
    input_hist = Input(shape=(look_back, 2), name='input_history')
    x1 = LSTM(128, return_sequences=False)(input_hist); x1 = Dropout(0.3)(x1)
    
    # --- Input 2: å•è¯éš¾åº¦ (Dense) ---
    input_diff = Input(shape=(1,), name='input_difficulty'); x2 = Dense(16, activation='relu')(input_diff)
    
    # --- Input 3: å•è¯ ID (Embedding) ---
    input_word = Input(shape=(1,), name='input_word_id'); x3 = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1)(input_word); x3 = Flatten()(x3)
    
    # --- Input 4: ç”¨æˆ·åå¥½ (Dense) ---
    input_bias = Input(shape=(1,), name='input_user_bias'); x4 = Dense(16, activation='relu')(input_bias) # æ–°å¢
    
    # --- èåˆå±‚ (Concatenate) ---
    combined = Concatenate()([x1, x2, x3, x4]) # èåˆå››ä¸ªåˆ†æ”¯
    
    z = Dense(64, activation='relu')(combined); z = Dropout(0.2)(z)
    
    # --- è¾“å‡ºå±‚ ---
    out_steps = Dense(1, name='output_steps', dtype='float32')(Dense(32, activation='relu')(z))
    out_success = Dense(1, activation='sigmoid', name='output_success', dtype='float32')(Dense(16, activation='relu')(z))
    
    # å¿…é¡»æ›´æ–° inputs åˆ—è¡¨
    model = Model(inputs=[input_hist, input_diff, input_word, input_bias], outputs=[out_steps, out_success])
    
    model.compile(optimizer='adam',
                  loss={'output_steps': 'mse', 'output_success': 'binary_crossentropy'},
                  loss_weights={'output_steps': 1.0, 'output_success': 0.5},
                  metrics={'output_success': 'accuracy'})
    return model

# ==========================================
# 3. éªŒè¯ä¸å›æµ‹é€»è¾‘ (ä¿®æ­£åçš„ç‰ˆæœ¬)
# ==========================================

def prepare_single_inference_input(history_tuples, target_diff, target_word_id, target_user_bias, look_back):
    """è¾…åŠ©å‡½æ•°ï¼šä¸ºå•æ¬¡é¢„æµ‹å‡†å¤‡å››è¾“å…¥å¼ é‡ã€‚"""
    trials = [h[0] for h in history_tuples]
    arr = np.array(trials)
    
    # æ„é€  Input 1 (History)
    std = np.std(arr) / 7.0; norm = arr / 7.0
    seq_2d = np.stack([norm, np.full_like(norm, std)], axis=1).reshape(1, look_back, 2)
    # æ„é€  Input 2, 3, 4 (Context)
    diff_in = np.array([target_diff / 7.0]).reshape(1, 1)
    word_in = np.array([target_word_id]).reshape(1, 1)
    bias_in = np.array([target_user_bias / 7.0]).reshape(1, 1)
    
    return [
        seq_2d.astype(np.float32), 
        diff_in.astype(np.float32), 
        word_in.astype(np.float32), 
        bias_in.astype(np.float32)
    ]

def evaluate_model_and_get_preds(model, val_inputs, val_labels):
    """è¿›è¡Œæ‰¹é‡é¢„æµ‹ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡ï¼Œå¹¶è®¡ç®—æ•´ä½“æŒ‡æ ‡ã€‚"""
    print("\nStep 4: å¼€å§‹æ‰¹é‡é¢„æµ‹ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡...")
    
    # æ‰¹é‡é¢„æµ‹ (verbose=1 å¼€å¯è¿›åº¦æ¡)
    predictions = model.predict(val_inputs, batch_size=BATCH_SIZE, verbose=1)
    
    pred_steps = predictions[0].flatten()
    pred_success_prob = predictions[1].flatten()
    
    # çœŸå®æ ‡ç­¾
    true_steps = val_labels['output_steps']
    
    # è®¡ç®—æŒ‡æ ‡
    mae = mean_absolute_error(true_steps, pred_steps.clip(max=7.0))
    
    return pred_steps.clip(max=6.99), pred_success_prob, mae

def perform_validation(model, user_history_map, valid_players, look_back, sample_size, threshold, pred_steps_full):
    
    buffer = io.StringIO()
    
    eligible = [u for u in valid_players if len(user_history_map[u]) >= look_back + 1]
    if len(eligible) < sample_size: sample_size = len(eligible)
    
    buffer.write(f"\nStep 5: å¯åŠ¨å›æµ‹éªŒè¯æŠ½æ ·æŠ¥å‘Š (æ ·æœ¬æ•°={sample_size}, è¾“å‡ºè‡³ output.txt)...")
    
    header = f"{'User ID':<10} | {'Bias':<5} | {'Diff':<5} | {'Real':<5} | {'Pred':<5} | {'Err':<5} | {'Status'}"
    buffer.write("\n" + "-" * 60 + "\n" + header + "\n" + "-" * 60)
    
    # éšæœºæŠ½å–ç”¨æˆ·æ¥å±•ç¤ºä»–ä»¬çš„æœ€åä¸€æ¬¡æ¸¸æˆé¢„æµ‹
    report_users = random.sample(eligible, min(10, sample_size))
    
    large_errors = 0
    
    for i, user in enumerate(report_users):
        full_hist = user_history_map[user]
        # è·å–ç›®æ ‡æ•°æ® (Trial, Difficulty, WordID, UserBias)
        target_data = full_hist[-1]
        
        real_trial = float(target_data[0])
        t_diff = target_data[1]; t_word = target_data[2]; t_bias = target_data[3]
        
        # ä¸´æ—¶è¿›è¡Œå•æ ·æœ¬é¢„æµ‹ä»¥è·å¾—è¯¥ç”¨æˆ·çš„é¢„æµ‹å€¼ (æ­¤æ­¥éª¤æ•ˆç‡ä½ï¼Œä»…ä¸ºç”ŸæˆæŠ¥å‘Šç¤ºä¾‹)
        temp_inputs = prepare_single_inference_input(full_hist[-(look_back+1) : -1], t_diff, t_word, t_bias, look_back)
        p_steps, _ = model.predict(temp_inputs, verbose=0)
        
        pred_val = min(p_steps[0][0], 6.99)
        err = abs(pred_val - real_trial)
        
        if err > threshold: large_errors += 1
        
        status = "âœ…" if err < 1.0 else "âš ï¸"
        if err > threshold: status = "âŒ"
        line = f"{str(user):<10} | {t_bias:.2f}  | {t_diff:.2f}  | {real_trial:.0f}    | {pred_val:.2f}  | {err:.2f}  | {status}"
        buffer.write(f"\n{line}")

    if sample_size > len(report_users):
        buffer.write(f"\n... (å…¶ä½™ {sample_size - len(report_users)} æ¡çœç•¥)")
    
    # æ‰“å°åˆ°æ§åˆ¶å°
    print(buffer.getvalue())
    
    # å†™å…¥æ–‡ä»¶
    with open("output.txt", "a", encoding="utf-8") as f: # æ”¹ä¸º 'a' ä»¥è¿½åŠ åˆ°å…¨å±€æŠ¥å‘Š
        f.write(buffer.getvalue())
    print("\nâœ… æŠ½æ ·æŠ¥å‘Šå·²æˆåŠŸå¯¼å‡ºè‡³ output.txt æ–‡ä»¶ã€‚")


# ==========================================
# 4. ä¸»ç¨‹åº (Main)
# ==========================================

def main():
    # --- GPU è®¾ç½® ---
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)
            set_global_policy('mixed_float16')
            print("âœ… GPU åŠ é€Ÿå·²å¼€å¯ (Mixed Float16)")
        except: pass
        
    # 1. æ•°æ®å¤„ç†
    user_map, vocab_size, _, _ = process_data_and_extract_features(FILE_PATH, MAX_ROWS)
    if not user_map: return 

    # 2. æ•°æ®é›†æ„å»º
    X_s, X_d, X_w, X_b, y_st, y_su, valid_users = create_multi_input_dataset(user_map, LOOK_BACK)
    
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    indices = np.arange(len(y_st))
    train_idx, val_idx = train_test_split(indices, test_size=0.1, random_state=42)
    
    # æ„å»º TF Dataset è¾…åŠ©å‡½æ•°
    def make_ds(idx):
        return tf.data.Dataset.from_tensor_slices((
            {
                'input_history': X_s[idx],
                'input_difficulty': X_d[idx],
                'input_word_id': X_w[idx],
                'input_user_bias': X_b[idx]
            },
            {
                'output_steps': y_st[idx],
                'output_success': y_su[idx]
            }
        )).shuffle(20000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
        
    train_ds = make_ds(train_idx)
    # val_ds ç”¨äºæ¨¡å‹è¯„ä¼°ï¼Œä¸éœ€è¦ shuffle
    val_ds = tf.data.Dataset.from_tensor_slices((
            {
                'input_history': X_s[val_idx],
                'input_difficulty': X_d[val_idx],
                'input_word_id': X_w[val_idx],
                'input_user_bias': X_b[val_idx]
            },
            {
                'output_steps': y_st[val_idx],
                'output_success': y_su[val_idx]
            }
        )).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    
    # 3. æ¨¡å‹æ„å»ºä¸è®­ç»ƒ/åŠ è½½
    is_trained = False
    model = None
    
    if os.path.exists(MODEL_SAVE_PATH):
        print(f"Step 3: æ£€æµ‹åˆ°å·²ä¿å­˜æ¨¡å‹ {MODEL_SAVE_PATH}ï¼Œå°è¯•åŠ è½½...")
        try:
            model = tf.keras.models.load_model(MODEL_SAVE_PATH)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè·³è¿‡è®­ç»ƒã€‚")
            is_trained = True
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}. å°†é‡æ–°æ„å»ºå¹¶è®­ç»ƒæ¨¡å‹ã€‚")
            is_trained = False
            
    if not is_trained:
        # æ„å»ºæ¨¡å‹
        model = build_context_model(LOOK_BACK, vocab_size, EMBEDDING_DIM)
        
        # è®­ç»ƒ
        print(f"Step 4A: å¼€å§‹è®­ç»ƒ (Epochs={EPOCHS}, Batch={BATCH_SIZE})...")
        model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=1)
        
        # è®­ç»ƒå®Œæˆåä¿å­˜
        try:
            model.save(MODEL_SAVE_PATH)
            print(f"\nâœ… æ¨¡å‹å·²æˆåŠŸä¿å­˜åˆ°æ–‡ä»¶å¤¹: {MODEL_SAVE_PATH}")
        except Exception as save_e:
            print(f"\nâŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {save_e}")
            
    # 4. æ‰¹é‡é¢„æµ‹å¹¶è·å–ç»“æœ (æ˜¾ç¤ºè¿›åº¦æ¡)
    val_inputs = {
        'input_history': X_s[val_idx],
        'input_difficulty': X_d[val_idx],
        'input_word_id': X_w[val_idx],
        'input_user_bias': X_b[val_idx]
    }
    val_labels = {
        'output_steps': y_st[val_idx],
        'output_success': y_su[val_idx]
    }

    pred_steps, pred_success_prob, mae = evaluate_model_and_get_preds(model, val_inputs, val_labels)

    # è®¡ç®— ACC
    true_wins = val_labels['output_success']
    pred_wins = (pred_success_prob >= 0.5).astype(int)
    acc = accuracy_score(true_wins, pred_wins)
    
    # 5. éªŒè¯æŠ½æ ·æŠ¥å‘Š (ä½¿ç”¨æ‰¹é‡ç»“æœè®¡ç®—çš„ MAE/ACC)
    
    # ç”ŸæˆæŠ¥å‘Šçš„å¤´éƒ¨å’Œæ±‡æ€»æŒ‡æ ‡
    report = f"""
========================================
 ğŸ“Š æœ€ç»ˆéªŒè¯æŠ¥å‘Š (Validation Report)
========================================
1. å¹³å‡æ­¥æ•°è¯¯å·® (MAE)    : {mae:.4f}
2. èƒœè´Ÿé¢„æµ‹å‡†ç¡®ç‡        : {acc:.1%}
3. å¤§å‹è¯¯å·®ç‡ (>{LARGE_ERROR_THRESHOLD}æ­¥)  : {np.mean(np.abs(val_labels['output_steps'] - pred_steps) > LARGE_ERROR_THRESHOLD):.1%}
========================================
"""
    # æ¸…ç©º output.txt å¹¶å†™å…¥å…¨å±€æŠ¥å‘Š
    with open("output.txt", "w", encoding="utf-8") as f:
        f.write(report)
    print(report)
    
    # è°ƒç”¨ perform_validation è¿›è¡ŒæŠ½æ ·æŠ¥å‘Š
    perform_validation(model, user_map, valid_users, LOOK_BACK, 
                       VALIDATION_SAMPLE_SIZE, LARGE_ERROR_THRESHOLD, pred_steps)

if __name__ == "__main__":
    main()