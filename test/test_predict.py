import unittest
import os
import sys
import json
import numpy as np
import pandas as pd
import tensorflow as tf
from unittest.mock import patch, mock_open, MagicMock

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from predict import (
    set_seed, TransformerBlock, focal_loss, safe_read_csv, load_tokenizer,
    encode_guess_sequence, attach_features, build_history, create_samples_lstm,
    create_samples_transformer, calculate_metrics
)

class TestPredict(unittest.TestCase):
    """æµ‹è¯•é¢„æµ‹æ¨¡å—"""
    
    def setUp(self):
        """æµ‹è¯•å‰çš„å‡†å¤‡å·¥ä½œ"""
        self.test_dir = 'test_predict_outputs'
        os.makedirs(self.test_dir, exist_ok=True)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        self.test_df = pd.DataFrame({
            'Username': ['user1', 'user1', 'user1', 'user1', 'user1', 'user1'],
            'target': ['apple', 'banana', 'cherry', 'date', 'elder', 'fig'],
            'Trial': [3, 4, 5, 6, 7, 2],
            'Game': [1, 2, 3, 4, 5, 6]
        })
        
        # åˆ›å»ºæµ‹è¯•tokenizer
        self.test_tokenizer = {
            'apple': 1,
            'banana': 2,
            'cherry': 3,
            'date': 4,
            'elder': 5,
            'fig': 6,
            '<OOV>': 0
        }
        
        # ç”¨æˆ·å’Œéš¾åº¦æ˜ å°„
        self.user_map = {'user1': 3.5}
        self.diff_map = {'apple': 4.0, 'banana': 3.0, 'cherry': 5.0, 'date': 3.5, 'elder': 4.5, 'fig': 2.5}
    
    def tearDown(self):
        """æµ‹è¯•åçš„æ¸…ç†å·¥ä½œ"""
        # åˆ é™¤æµ‹è¯•ç›®å½•
        if os.path.exists(self.test_dir):
            for file in os.listdir(self.test_dir):
                file_path = os.path.join(self.test_dir, file)
                os.remove(file_path)
            os.rmdir(self.test_dir)
    
    def test_set_seed(self):
        """æµ‹è¯•éšæœºç§å­è®¾ç½®åŠŸèƒ½"""
        # è®¾ç½®ä¸åŒç§å­å¹¶ç”Ÿæˆéšæœºæ•°
        set_seed(42)
        val1 = np.random.rand()
        
        set_seed(42)
        val2 = np.random.rand()
        
        set_seed(123)
        val3 = np.random.rand()
        
        # éªŒè¯ç›¸åŒç§å­ç”Ÿæˆç›¸åŒç»“æœ
        self.assertEqual(val1, val2)
        # éªŒè¯ä¸åŒç§å­ç”Ÿæˆä¸åŒç»“æœ
        self.assertNotEqual(val1, val3)
    
    def test_transformer_block(self):
        """æµ‹è¯•TransformerBlockå±‚"""
        # åˆ›å»ºTransformerBlockå®ä¾‹
        embed_dim = 64
        num_heads = 8
        ff_dim = 128
        transformer_block = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        input_data = tf.random.normal(shape=(32, 5, embed_dim))
        output = transformer_block(input_data)
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        self.assertEqual(output.shape, (32, 5, embed_dim))
    
    def test_focal_loss(self):
        """æµ‹è¯•focal_losså‡½æ•°"""
        # åˆ›å»ºfocal_losså®ä¾‹
        loss_fn = focal_loss(gamma=2.0, alpha=0.25)
        
        # æµ‹è¯•æŸå¤±è®¡ç®—
        y_true = tf.constant([1.0, 0.0, 1.0, 0.0])
        y_pred = tf.constant([0.9, 0.1, 0.8, 0.2])
        loss = loss_fn(y_true, y_pred)
        
        # éªŒè¯æŸå¤±å€¼æ˜¯ä¸€ä¸ªæ ‡é‡
        self.assertEqual(tf.rank(loss).numpy(), 0)
    
    def test_safe_read_csv(self):
        """æµ‹è¯•å®‰å…¨è¯»å–CSVæ–‡ä»¶åŠŸèƒ½"""
        # æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨çš„æƒ…å†µ
        df_missing = safe_read_csv('non_existent_file.csv')
        self.assertTrue(df_missing.empty)
        
        # æµ‹è¯•æ–‡ä»¶å­˜åœ¨çš„æƒ…å†µ
        test_csv = os.path.join(self.test_dir, 'test.csv')
        self.test_df.to_csv(test_csv, index=False)
        
        df_exist = safe_read_csv(test_csv)
        self.assertEqual(len(df_exist), len(self.test_df))
    
    @patch('predict.os.path.exists')
    @patch('predict.open', new_callable=mock_open, read_data=json.dumps({'apple': 1, 'banana': 2, '<OOV>': 0}))
    def test_load_tokenizer(self, mock_file, mock_exists):
        """æµ‹è¯•åŠ è½½tokenizeråŠŸèƒ½"""
        # è®¾ç½®mock
        mock_exists.return_value = True
        
        # è°ƒç”¨å‡½æ•°
        tokenizer = load_tokenizer('test_tokenizer.json')
        
        # éªŒè¯ç»“æœ
        self.assertIsNotNone(tokenizer)
        self.assertEqual(tokenizer.word_index['apple'], 1)
        mock_exists.assert_called_once_with('test_tokenizer.json')
    
    def test_encode_guess_sequence(self):
        """æµ‹è¯•çŒœæµ‹åºåˆ—ç¼–ç åŠŸèƒ½"""
        # æµ‹è¯•æœ‰æ•ˆçš„çŒœæµ‹åºåˆ—
        grid_cell = "['ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©', 'ğŸŸ©ğŸŸ¨â¬œâ¬œâ¬œ', 'ğŸŸ©ğŸŸ¨ğŸŸ¨â¬œâ¬œ']"
        sequence = encode_guess_sequence(grid_cell)
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        self.assertEqual(sequence.shape, (6, 8))
        
        # æµ‹è¯•ç©ºå€¼æƒ…å†µ
        sequence_null = encode_guess_sequence(None)
        self.assertEqual(sequence_null.shape, (6, 8))
        
        # æµ‹è¯•æ— æ•ˆæ ¼å¼
        sequence_invalid = encode_guess_sequence('invalid_format')
        self.assertEqual(sequence_invalid.shape, (6, 8))
    
    @patch('predict.Tokenizer')
    def test_attach_features(self, mock_tokenizer_class):
        """æµ‹è¯•ç‰¹å¾é™„åŠ åŠŸèƒ½"""
        # è®¾ç½®mock
        mock_tokenizer = MagicMock()
        mock_tokenizer.texts_to_sequences.return_value = [[1], [2], [3], [4], [5], [6]]
        mock_tokenizer_class.return_value = mock_tokenizer
        
        # è°ƒç”¨å‡½æ•°
        result_df = attach_features(self.test_df, mock_tokenizer, self.user_map, self.diff_map)
        
        # éªŒè¯ç»“æœ
        self.assertIn('word_id', result_df.columns)
        self.assertIn('word_difficulty', result_df.columns)
        self.assertIn('user_bias', result_df.columns)
        self.assertIn('grid_seq_processed', result_df.columns)
    
    def test_build_history(self):
        """æµ‹è¯•æ„å»ºå†å²è®°å½•åŠŸèƒ½"""
        # æ·»åŠ build_historyå‡½æ•°æ‰€éœ€çš„æ‰€æœ‰åˆ—
        self.test_df['word_id'] = [1, 2, 3, 4, 5, 6]  # æ·»åŠ word_idåˆ—
        self.test_df['user_bias'] = [3.5, 3.5, 3.5, 3.5, 3.5, 3.5]  # æ·»åŠ user_biasåˆ—
        self.test_df['word_difficulty'] = [4.0, 3.0, 5.0, 3.5, 4.5, 2.5]  # æ·»åŠ word_difficultyåˆ—
        self.test_df['grid_seq_processed'] = [np.zeros((6, 8)) for _ in range(len(self.test_df))]  # æ·»åŠ grid_seq_processedåˆ—
        
        # è°ƒç”¨å‡½æ•°
        history = build_history(self.test_df)
        
        # éªŒè¯ç»“æœ
        self.assertIsInstance(history, dict)
        self.assertIn('user1', history)
        self.assertEqual(len(history['user1']), len(self.test_df))
    
    def test_calculate_metrics(self):
        """æµ‹è¯•æŒ‡æ ‡è®¡ç®—åŠŸèƒ½"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        y_true_steps = np.array([3.0, 4.0, 5.0, 6.0, 7.0, 2.0])
        y_true_succ = np.array([1.0, 1.0, 1.0, 1.0, 0.0, 1.0])
        pred_steps = np.array([3.2, 4.1, 4.8, 6.3, 6.7, 2.2])
        pred_prob = np.array([0.8, 0.9, 0.7, 0.85, 0.3, 0.95])
        threshold = 0.69
        
        # è°ƒç”¨å‡½æ•°
        metrics = calculate_metrics(y_true_steps, y_true_succ, pred_steps, pred_prob, threshold)
        
        # éªŒè¯ç»“æœ
        self.assertIn('MAE', metrics)
        self.assertIn('RMSE', metrics)
        self.assertIn('ACC', metrics)
        self.assertIn('AUC', metrics)
        self.assertIn('CM', metrics)
        
        # éªŒè¯æŒ‡æ ‡å€¼çš„åˆç†æ€§
        self.assertGreater(metrics['MAE'], 0)
        self.assertGreater(metrics['RMSE'], 0)
        self.assertLessEqual(metrics['ACC'], 1.0)
        self.assertLessEqual(metrics['AUC'], 1.0)

if __name__ == '__main__':
    unittest.main()