import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, Flatten, Concatenate, MultiHeadAttention, LayerNormalization
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, roc_auc_score
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
from sklearn.model_selection import train_test_split
from tensorflow.keras.mixed_precision import set_global_policy
import random
import os
import io
import wandb
from wandb.keras import WandbCallback

# WandBå…¨å±€æ§åˆ¶å˜é‡
WANDB_ENABLED = True  # æ§åˆ¶æ˜¯å¦å¯ç”¨WandB
wandb_run = None  # å­˜å‚¨wandb runå®ä¾‹
import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

# ==========================================
# 0. ç”¨æˆ·é…ç½®å‚æ•° (STRICT CONFIG)
# ==========================================
# æ•°æ®é…ç½®
FILE_PATH = 'dataset/cleaned_dataset.csv'
MAX_ROWS = 6923127
LOOK_BACK = 8       # å†å²çª—å£å¤§å°

# æ¨¡å‹ç»“æ„é…ç½®
# Transformer æ ¸å¿ƒå‚æ•°
NUM_HEADS = 6      # å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
KEY_DIM = 36       # é”®å’ŒæŸ¥è¯¢çš„ç»´åº¦
FF_DIM = 108        # å‰é¦ˆç½‘ç»œçš„ç»´åº¦

# æ¨¡å‹å¤æ‚åº¦å‚æ•°
TRANSFORMER_LAYERS = 2  # Transformerå±‚çš„æ•°é‡
DROPOUT_RATE = 0.3   # Dropoutæ¯”ç‡
EMBEDDING_DIM = 64   # è¯åµŒå…¥ç»´åº¦

# è®­ç»ƒé…ç½®
EPOCHS = 10         
BATCH_SIZE = 2048    
LEARNING_RATE = 0.001 # å­¦ä¹ ç‡
PATIENCE = 4         # æ—©åœè€å¿ƒå€¼

# è¯„ä¼°é…ç½®
LARGE_ERROR_THRESHOLD = 2.0  # å¤§å‹è¯¯å·®çš„é˜ˆå€¼ï¼ˆæ­¥ï¼‰

# å…¶ä»–é…ç½®
MODEL_SAVE_PATH = 'Transformer_Model'

# WandB é…ç½®
WANDB_PROJECT = 'wordle-prediction'
WANDB_RUN_NAME = 'transformer-experiment'
WANDB_ENABLED = True  # æ§åˆ¶æ˜¯å¦å¯ç”¨WandB
wandb_run = None  # å­˜å‚¨wandb runå®ä¾‹

# ==========================================
# 1. æ•°æ®åŠ è½½ä¸é«˜çº§ç‰¹å¾å·¥ç¨‹
# ==========================================

def process_data_and_extract_features(file_path, nrows):
    """è¯»å–æ•°æ®ï¼Œè®¡ç®—å•è¯éš¾åº¦ã€å•è¯IDã€ç”¨æˆ·å¹³å‡åå¥½ã€‚"""
    print("Step 1: è¯»å–æ•°æ®å¹¶æ„å»ºå››ç‰¹å¾...")
    try:
        df = pd.read_csv(file_path, nrows=nrows, usecols=['Game', 'Trial', 'Username', 'target'])
        df = df.dropna()
    except:
        # æ¨¡æ‹Ÿæ•°æ®... (çœç•¥æ¨¡æ‹Ÿé€»è¾‘ï¼Œè§å‰ä¸€ä¸ªç‰ˆæœ¬)
        print("âš ï¸ æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿ wordle_games.csv å­˜åœ¨ã€‚")
        return {}, 2, None, None 

    # --- Feature A & B: å•è¯éš¾åº¦ (Difficulty) & å•è¯ ID (Embedding) ---
    word_stats = df.groupby('target')['Trial'].mean().to_dict()
    df['word_difficulty'] = df['target'].map(word_stats)
    tokenizer = Tokenizer(); tokenizer.fit_on_texts(df['target'])
    df['word_id'] = df['target'].apply(lambda x: tokenizer.texts_to_sequences([x])[0][0])
    vocab_size = len(tokenizer.word_index) + 1

    # --- Feature C: ç”¨æˆ·åå¥½ (User Bias) ---
    # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„å…¨å±€å¹³å‡æ­¥æ•°
    user_stats = df.groupby('Username')['Trial'].mean().to_dict()
    df['user_bias'] = df['Username'].map(user_stats)

    df = df.sort_values(by=['Username', 'Game'])
    
    # æ„å»ºå¤åˆå­—å…¸ï¼š(Trial, Difficulty, WordID, UserBias)
    history_map = df.groupby('Username').apply(
        lambda x: list(zip(x['Trial'], x['word_difficulty'], x['word_id'], x['user_bias']))
    ).to_dict()
    
    return history_map, vocab_size, tokenizer, word_stats

def create_multi_input_dataset(user_history_map, look_back):
    """æ„å»ºå››è¾“å…¥æ•°æ®é›†ï¼šX_seq, X_diff, X_word, X_bias."""
    print("Step 2: æ„å»ºå››è¾“å…¥æ ·æœ¬...")
    
    X_seq_list = []
    X_diff_list = []; X_word_list = []; X_bias_list = []
    y_steps = []; y_success = []
    valid_players = []

    for user, history in user_history_map.items():
        if len(history) > look_back:
            for i in range(len(history) - look_back):
                
                # --- 1. å†å²åºåˆ—ç‰¹å¾ (Transformer Input) ---
                past_trials = [h[0] for h in history[i : i+look_back]]
                past_arr = np.array(past_trials)
                std_dev = np.std(past_arr) / 7.0; seq_norm = past_arr / 7.0
                seq_2d = np.stack([seq_norm, np.full_like(seq_norm, std_dev)], axis=1)
                
                # --- 2. ç›®æ ‡ä¿¡æ¯å’Œç”¨æˆ·åå¥½ (Context Inputs) ---
                target_game = history[i + look_back]
                target_trial = target_game[0]
                target_difficulty = target_game[1]
                target_word_id = target_game[2]
                target_user_bias = target_game[3] # æ–°å¢
                
                # æ”¶é›†æ•°æ®
                X_seq_list.append(seq_2d)
                X_diff_list.append(target_difficulty / 7.0) 
                X_word_list.append(target_word_id)
                X_bias_list.append(target_user_bias / 7.0) # æ–°å¢
                
                # æ ‡ç­¾ï¼šå°†æ­¥æ•°è½¬æ¢ä¸º0-6çš„ç´¢å¼•ï¼ˆå¯¹åº”1-7æ­¥ï¼‰ï¼Œç„¶åè¿›è¡Œone-hotç¼–ç 
                step_idx = min(int(target_trial) - 1, 6)  # 1-7æ­¥è½¬æ¢ä¸º0-6ç´¢å¼•ï¼Œç¡®ä¿7æ­¥å¯¹åº”ç´¢å¼•6
                y_steps.append(step_idx)
                y_success.append(1.0 if target_trial <= 6 else 0.0)
            
            valid_players.append(user)

    return (
        np.array(X_seq_list, dtype=np.float32),
        np.array(X_diff_list, dtype=np.float32),
        np.array(X_word_list, dtype=np.float32),
        np.array(X_bias_list, dtype=np.float32), # æ–°å¢è¾“å‡º
        to_categorical(np.array(y_steps, dtype=np.int32), num_classes=7),  # è½¬æ¢ä¸ºone-hotç¼–ç 
        np.array(y_success, dtype=np.float32),
        valid_players
    )

# ==========================================
# 2. Transformer æ¨¡å‹æ„å»º (Multi-Input Four Branch)
# ==========================================

class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation="relu"), 
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

def build_transformer_model(look_back, vocab_size, embedding_dim, num_heads, key_dim, ff_dim, transformer_layers=1, dropout_rate=0.3, learning_rate=0.001):
    print(f"Step 3: æ„å»ºTransformerç¥ç»ç½‘ç»œ (Vocab={vocab_size}, Layers={transformer_layers})...")
    
    # --- Input 1: ç©å®¶å†å² (Transformer) ---
    input_hist = Input(shape=(look_back, 2), name='input_history')
    # æ·»åŠ ä½ç½®ç¼–ç ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    positions = tf.range(start=0, limit=look_back, delta=1)
    pos_encoding = Embedding(input_dim=look_back, output_dim=2)(positions)
    pos_encoding = tf.expand_dims(pos_encoding, 0)
    x1 = input_hist + pos_encoding  # æ·»åŠ ä½ç½®ç¼–ç 
    
    # åº”ç”¨å¤šä¸ªTransformerå±‚
    for i in range(transformer_layers):
        transformer_block = TransformerBlock(embed_dim=2, num_heads=num_heads, ff_dim=ff_dim, rate=dropout_rate)
        x1 = transformer_block(x1)
    
    x1 = tf.keras.layers.GlobalAveragePooling1D()(x1)  # æ± åŒ–ä¸ºå›ºå®šé•¿åº¦å‘é‡
    x1 = Dropout(dropout_rate)(x1)
    
    # --- Input 2: å•è¯éš¾åº¦ (Dense) ---
    input_diff = Input(shape=(1,), name='input_difficulty'); 
    x2 = Dense(16, activation='relu')(input_diff)
    
    # --- Input 3: å•è¯ ID (Embedding) ---
    input_word = Input(shape=(1,), name='input_word_id'); 
    x3 = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1)(input_word); 
    x3 = Flatten()(x3)
    
    # --- Input 4: ç”¨æˆ·åå¥½ (Dense) ---
    input_bias = Input(shape=(1,), name='input_user_bias'); 
    x4 = Dense(16, activation='relu')(input_bias) # æ–°å¢
    
    # --- èåˆå±‚ (Concatenate) ---
    combined = Concatenate()([x1, x2, x3, x4]) # èåˆå››ä¸ªåˆ†æ”¯
    
    z = Dense(64, activation='relu')(combined); 
    z = Dropout(dropout_rate)(z)
    
    # --- è¾“å‡ºå±‚ ---
    # å°†æ­¥æ•°é¢„æµ‹ä»å›å½’æ”¹ä¸ºåˆ†ç±»ä»»åŠ¡ï¼š7ä¸ªç±»åˆ«(1-7æ­¥)
    out_steps = Dense(7, activation='softmax', name='output_steps', dtype='float32')(Dense(32, activation='relu')(z))
    out_success = Dense(1, activation='sigmoid', name='output_success', dtype='float32')(Dense(16, activation='relu')(z))
    
    # å¿…é¡»æ›´æ–° inputs åˆ—è¡¨
    model = Model(inputs=[input_hist, input_diff, input_word, input_bias], outputs=[out_steps, out_success])
    
    # ä½¿ç”¨æŒ‡å®šçš„å­¦ä¹ ç‡
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    
    model.compile(optimizer=optimizer,
                  loss={'output_steps': 'categorical_crossentropy', 'output_success': 'binary_crossentropy'},
                  loss_weights={'output_steps': 1.0, 'output_success': 0.5},
                  metrics={'output_success': 'accuracy', 'output_steps': 'accuracy'})
    return model

# ==========================================
# 3. éªŒè¯ä¸å›æµ‹é€»è¾‘
# ==========================================

def prepare_single_inference_input(history_tuples, target_diff, target_word_id, target_user_bias, look_back):
    """è¾…åŠ©å‡½æ•°ï¼šä¸ºå•æ¬¡é¢„æµ‹å‡†å¤‡å››è¾“å…¥å¼ é‡ã€‚"""
    trials = [h[0] for h in history_tuples]
    arr = np.array(trials)
    
    # æ„é€  Input 1 (History)
    std = np.std(arr) / 7.0; norm = arr / 7.0
    seq_2d = np.stack([norm, np.full_like(norm, std)], axis=1).reshape(1, look_back, 2)
    # æ„é€  Input 2, 3, 4 (Context)
    diff_in = np.array([target_diff / 7.0]).reshape(1, 1)
    word_in = np.array([target_word_id]).reshape(1, 1)
    bias_in = np.array([target_user_bias / 7.0]).reshape(1, 1)
    
    return [
        seq_2d.astype(np.float32), 
        diff_in.astype(np.float32), 
        word_in.astype(np.float32), 
        bias_in.astype(np.float32)
    ]

def evaluate_model_and_get_preds(model, val_inputs, val_labels):
    """è¿›è¡Œæ‰¹é‡é¢„æµ‹ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡ï¼Œå¹¶è®¡ç®—åˆ†ç±»æŒ‡æ ‡ã€‚"""
    print("\nStep 4: å¼€å§‹æ‰¹é‡é¢„æµ‹ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡...")
    
    # æ‰¹é‡é¢„æµ‹ (verbose=1 å¼€å¯è¿›åº¦æ¡)
    predictions = model.predict(val_inputs, batch_size=BATCH_SIZE, verbose=1)
    
    # å¤„ç†åˆ†ç±»è¾“å‡ºï¼šè·å–æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ç´¢å¼•
    pred_steps_probs = predictions[0]  # å½¢çŠ¶ä¸º(batch_size, 7)çš„æ¦‚ç‡åˆ†å¸ƒ
    pred_steps_discrete = np.argmax(pred_steps_probs, axis=1) + 1  # å°†0-6ç´¢å¼•è½¬æ¢ä¸º1-7æ­¥
    
    # ç”Ÿæˆè¿ç»­é¢„æµ‹å€¼ç”¨äºå¯è§†åŒ–ï¼ˆå–æ¦‚ç‡åŠ æƒå¹³å‡å€¼ï¼‰
    step_values = np.arange(1, 8)  # [1,2,3,4,5,6,7]
    pred_steps_continuous = np.sum(pred_steps_probs * step_values, axis=1)
    
    pred_success_prob = predictions[1].flatten()
    
    return pred_steps_continuous, pred_steps_discrete, pred_success_prob

def perform_validation(model, user_history_map, valid_players, look_back, sample_size, threshold, pred_steps_full):
    
    buffer = io.StringIO()
    
    eligible = [u for u in valid_players if len(user_history_map[u]) >= look_back + 1]
    if len(eligible) < sample_size: sample_size = len(eligible)
    
    buffer.write(f"\nStep 5: å¯åŠ¨å›æµ‹éªŒè¯æŠ½æ ·æŠ¥å‘Š (æ ·æœ¬æ•°={sample_size}, è¾“å‡ºè‡³ output.txt)...")
    
    header = f"{'User ID':<10} | {'Bias':<5} | {'Diff':<5} | {'Real':<5} | {'Pred_Cont':<8} | {'Pred_Disc':<5} | {'Err':<5} | {'Status'}"
    buffer.write("\n" + "-" * 75 + "\n" + header + "\n" + "-" * 75)
    
    # éšæœºæŠ½å–ç”¨æˆ·æ¥å±•ç¤ºä»–ä»¬çš„æœ€åä¸€æ¬¡æ¸¸æˆé¢„æµ‹
    report_users = random.sample(eligible, min(10, sample_size))
    
    large_errors = 0
    
    for i, user in enumerate(report_users):
        full_hist = user_history_map[user]
        # è·å–ç›®æ ‡æ•°æ® (Trial, Difficulty, WordID, UserBias)
        target_data = full_hist[-1]
        
        real_trial = float(target_data[0])
        t_diff = target_data[1]; t_word = target_data[2]; t_bias = target_data[3]
        
        # ä¸´æ—¶è¿›è¡Œå•æ ·æœ¬é¢„æµ‹ä»¥è·å¾—è¯¥ç”¨æˆ·çš„é¢„æµ‹å€¼ (æ­¤æ­¥éª¤æ•ˆç‡ä½ï¼Œä»…ä¸ºç”ŸæˆæŠ¥å‘Šç¤ºä¾‹)
        temp_inputs = prepare_single_inference_input(full_hist[-(look_back+1) : -1], t_diff, t_word, t_bias, look_back)
        p_steps_probs, _ = model.predict(temp_inputs, verbose=0)
        
        # å¤„ç†åˆ†ç±»è¾“å‡º
        pred_val_disc = np.argmax(p_steps_probs[0]) + 1  # å°†0-6ç´¢å¼•è½¬æ¢ä¸º1-7æ­¥
        
        # ç”Ÿæˆè¿ç»­é¢„æµ‹å€¼ï¼ˆæ¦‚ç‡åŠ æƒå¹³å‡ï¼‰
        step_values = np.arange(1, 8)  # [1,2,3,4,5,6,7]
        pred_val_cont = np.sum(p_steps_probs[0] * step_values)
        
        err = abs(pred_val_cont - real_trial)
        
        if err > threshold: large_errors += 1
        
        status = "âœ…" if err < 1.0 else "âš ï¸"
        if err > threshold: status = "âŒ"
        line = f"{str(user):<10} | {t_bias:.2f}  | {t_diff:.2f}  | {real_trial:.0f}    | {pred_val_cont:.2f}     | {pred_val_disc}      | {err:.2f}  | {status}"
        buffer.write(f"\n{line}")

    if sample_size > len(report_users):
        buffer.write(f"\n... (å…¶ä½™ {sample_size - len(report_users)} æ¡çœç•¥)")
    
    # æ‰“å°åˆ°æ§åˆ¶å°
    print(buffer.getvalue())
    
    # å†™å…¥æ–‡ä»¶
    with open("outputs/transformer_output.txt", "a", encoding="utf-8") as f: 
        f.write(buffer.getvalue())
    print("\nâœ… æŠ½æ ·æŠ¥å‘Šå·²æˆåŠŸå¯¼å‡ºè‡³ outputs/transformer_output.txt æ–‡ä»¶ã€‚")

# ==========================================
# 4. ä¸»ç¨‹åº (Main)
# ==========================================

def plot_loss_curve(history, model_name, save_dir):
    """ç»˜åˆ¶å¹¶ä¿å­˜Lossæ›²çº¿"""
    plt.figure(figsize=(12, 6))
    
    # ç»˜åˆ¶æ€»æŸå¤±æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='è®­ç»ƒæŸå¤±')
    plt.plot(history.history['val_loss'], label='éªŒè¯æŸå¤±')
    plt.title(f'{model_name} æ€»æŸå¤±æ›²çº¿')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    # ç»˜åˆ¶æ­¥æ•°é¢„æµ‹æŸå¤±æ›²çº¿ - åˆ†ç±»ä»»åŠ¡ä½¿ç”¨äº¤å‰ç†µæŸå¤±
    plt.subplot(1, 2, 2)
    plt.plot(history.history['output_steps_loss'], label='è®­ç»ƒæ­¥æ•°æŸå¤±')
    plt.plot(history.history['val_output_steps_loss'], label='éªŒè¯æ­¥æ•°æŸå¤±')
    plt.title(f'{model_name} æ­¥æ•°é¢„æµ‹æŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('Cross-Entropy Loss')
    plt.legend()
    plt.grid(True)
    
    # å°è¯•æ·»åŠ æ­¥æ•°é¢„æµ‹çš„å‡†ç¡®ç‡æ›²çº¿
    if 'output_steps_accuracy' in history.history and 'val_output_steps_accuracy' in history.history:
        plt.figure(figsize=(12, 6))
        plt.plot(history.history['output_steps_accuracy'], label='è®­ç»ƒæ­¥æ•°å‡†ç¡®ç‡')
        plt.plot(history.history['val_output_steps_accuracy'], label='éªŒè¯æ­¥æ•°å‡†ç¡®ç‡')
        plt.title(f'{model_name} æ­¥æ•°é¢„æµ‹å‡†ç¡®ç‡')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        
        # ä¿å­˜å‡†ç¡®ç‡æ›²çº¿
        acc_path = os.path.join(save_dir, f'{model_name}_accuracy_curve.png')
        plt.savefig(acc_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… å‡†ç¡®ç‡æ›²çº¿å·²ä¿å­˜è‡³: {acc_path}")
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, f'{model_name}_loss_curve.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… Lossæ›²çº¿å·²ä¿å­˜è‡³: {save_path}")
    return save_path

def plot_prediction_trends(true_steps, pred_steps_cont, pred_steps_disc, model_name, save_dir):
    """ç»˜åˆ¶é¢„æµ‹ç»“æœè¶‹åŠ¿å›¾"""
    # éšæœºé€‰æ‹©100ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–ï¼Œé¿å…å›¾è¿‡äºæ‹¥æŒ¤
    if len(true_steps) > 100:
        indices = np.random.choice(len(true_steps), 100, replace=False)
        true_sample = true_steps[indices]
        pred_sample_cont = pred_steps_cont[indices]
        pred_sample_disc = pred_steps_disc[indices]
    else:
        true_sample = true_steps
        pred_sample_cont = pred_steps_cont
        pred_sample_disc = pred_steps_disc
    
    # æŒ‰çœŸå®å€¼æ’åº
    sorted_indices = np.argsort(true_sample)
    true_sample_sorted = true_sample[sorted_indices]
    pred_sample_cont_sorted = pred_sample_cont[sorted_indices]
    pred_sample_disc_sorted = pred_sample_disc[sorted_indices]
    
    plt.figure(figsize=(15, 6))
    
    # ç»˜åˆ¶é¢„æµ‹å€¼ä¸çœŸå®å€¼å¯¹æ¯”å›¾
    plt.subplot(1, 3, 1)
    plt.scatter(true_sample, pred_sample_cont, alpha=0.5, s=50, label='è¿ç»­é¢„æµ‹')
    plt.scatter(true_sample, pred_sample_disc, alpha=0.5, s=50, c='green', label='ç¦»æ•£é¢„æµ‹')
    plt.plot([0, 7], [0, 7], 'r--', lw=2)  # ç†æƒ³çº¿
    plt.title(f'{model_name} é¢„æµ‹å€¼ vs çœŸå®å€¼')
    plt.xlabel('çœŸå®æ­¥æ•°')
    plt.ylabel('é¢„æµ‹æ­¥æ•°')
    plt.legend()
    plt.grid(True)
    plt.xlim(0, 7)
    plt.ylim(0, 7)
    
    # ç»˜åˆ¶æ’åºåçš„é¢„æµ‹è¶‹åŠ¿ï¼ˆè¿ç»­å€¼ï¼‰
    plt.subplot(1, 3, 2)
    plt.plot(range(len(true_sample_sorted)), true_sample_sorted, 'b-', label='çœŸå®å€¼')
    plt.plot(range(len(pred_sample_cont_sorted)), pred_sample_cont_sorted, 'r--', label='è¿ç»­é¢„æµ‹')
    plt.title(f'{model_name} è¿ç»­é¢„æµ‹è¶‹åŠ¿')
    plt.xlabel('æ ·æœ¬ç´¢å¼• (æŒ‰çœŸå®å€¼æ’åº)')
    plt.ylabel('æ­¥æ•°')
    plt.legend()
    plt.grid(True)
    plt.ylim(0, 7)
    
    # ç»˜åˆ¶æ’åºåçš„é¢„æµ‹è¶‹åŠ¿ï¼ˆç¦»æ•£å€¼ï¼‰
    plt.subplot(1, 3, 3)
    plt.plot(range(len(true_sample_sorted)), true_sample_sorted, 'b-', label='çœŸå®å€¼')
    plt.plot(range(len(pred_sample_disc_sorted)), pred_sample_disc_sorted, 'g--', label='ç¦»æ•£é¢„æµ‹ (1-7)')
    plt.title(f'{model_name} ç¦»æ•£é¢„æµ‹è¶‹åŠ¿')
    plt.xlabel('æ ·æœ¬ç´¢å¼• (æŒ‰çœŸå®å€¼æ’åº)')
    plt.ylabel('æ­¥æ•°')
    plt.legend()
    plt.grid(True)
    plt.ylim(0, 7)
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, f'{model_name}_prediction_trends.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… é¢„æµ‹è¶‹åŠ¿å›¾å·²ä¿å­˜è‡³: {save_path}")
    return save_path

def plot_confusion_matrix(true_steps, pred_steps_disc, model_name, save_dir):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶ä¿å­˜"""
    # ç¡®ä¿çœŸå®å€¼ä¹Ÿè½¬æ¢ä¸ºæ•´æ•°
    true_steps_disc = true_steps.astype(int)
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(true_steps_disc, pred_steps_disc)
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=[str(i) for i in range(1, 8)],
                yticklabels=[str(i) for i in range(1, 8)])
    plt.title(f'{model_name} é¢„æµ‹æ­¥æ•°æ··æ·†çŸ©é˜µ (1-7)')
    plt.xlabel('é¢„æµ‹æ­¥æ•°')
    plt.ylabel('çœŸå®æ­¥æ•°')
    plt.tight_layout()
    
    # ä¿å­˜æ··æ·†çŸ©é˜µ
    save_path = os.path.join(save_dir, f'{model_name}_confusion_matrix.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {save_path}")
    return save_path

def predict_with_model(model, user_history_map, user_id, look_back):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
    
    å‚æ•°:
    model: è®­ç»ƒå¥½çš„æ¨¡å‹
    user_history_map: ç”¨æˆ·å†å²æ•°æ®æ˜ å°„
    user_id: è¦é¢„æµ‹çš„ç”¨æˆ·ID
    look_back: å†å²çª—å£å¤§å°
    
    è¿”å›:
    è¿ç»­é¢„æµ‹æ­¥æ•°ã€ç¦»æ•£é¢„æµ‹æ­¥æ•°(1-7)å’ŒæˆåŠŸæ¦‚ç‡
    """
    if user_id not in user_history_map or len(user_history_map[user_id]) < look_back + 1:
        print(f"âš ï¸ ç”¨æˆ· {user_id} æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹")
        return None, None, None
    
    full_hist = user_history_map[user_id]
    # è·å–ç›®æ ‡æ•°æ® (Trial, Difficulty, WordID, UserBias)
    target_data = full_hist[-1]
    t_diff = target_data[1]; t_word = target_data[2]; t_bias = target_data[3]
    
    # å‡†å¤‡è¾“å…¥æ•°æ®
    inputs = prepare_single_inference_input(full_hist[-(look_back+1) : -1], t_diff, t_word, t_bias, look_back)
    pred_steps, pred_success = model.predict(inputs, verbose=0)
    
    # è¿ç»­é¢„æµ‹å€¼
    pred_steps_cont = min(pred_steps[0][0], 6.99)
    # ç¦»æ•£é¢„æµ‹å€¼ï¼ˆ1-7æ•´æ•°ï¼‰
    pred_steps_disc = int(round(pred_steps_cont))
    pred_steps_disc = max(1, min(7, pred_steps_disc))  # ç¡®ä¿åœ¨1-7èŒƒå›´å†…
    
    return pred_steps_cont, pred_steps_disc, pred_success[0][0]

def main(mode='train', user_id=None):
    """
    ä¸»å‡½æ•°
    
    å‚æ•°:
    mode: 'train' æˆ– 'predict'
    user_id: å½“modeä¸º'predict'æ—¶ï¼ŒæŒ‡å®šè¦é¢„æµ‹çš„ç”¨æˆ·ID
    """
    global WANDB_ENABLED, wandb_run
    if mode == 'train':
        # --- GPU è®¾ç½® ---
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            try:
                for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)
                set_global_policy('mixed_float16')
                print("âœ… GPU åŠ é€Ÿå·²å¼€å¯ (Mixed Float16)")
            except: pass
        
        # ç¡®ä¿å¯è§†åŒ–æ–‡ä»¶å¤¹å­˜åœ¨
        visualization_dir = 'visualization'
        os.makedirs(visualization_dir, exist_ok=True)
        os.makedirs('outputs', exist_ok=True)
        
        # --- åˆå§‹åŒ– WandB ---
        global wandb_run
        if WANDB_ENABLED:
            print("ğŸ”„ åˆå§‹åŒ– WandB å®éªŒè®°å½•...")
            try:
                # ä½¿ç”¨ç¦»çº¿æ¨¡å¼é¿å…ç½‘ç»œè¿æ¥é—®é¢˜
                wandb_run = wandb.init(
                    project=WANDB_PROJECT, 
                    name=WANDB_RUN_NAME, 
                    dir='wandb', 
                    anonymous='must',
                    resume=False,
                    mode='offline',  # æ·»åŠ ç¦»çº¿æ¨¡å¼
                    settings=wandb.Settings(
                        start_method='thread',
                        disable_git=True,
                        disable_code=True
                    )
                )
                # è®°å½•è¶…å‚æ•°
                config = wandb_run.config
                config.look_back = LOOK_BACK
                config.epochs = EPOCHS
                config.batch_size = BATCH_SIZE
                config.embedding_dim = EMBEDDING_DIM
                config.num_heads = NUM_HEADS
                config.ff_dim = FF_DIM
                config.transformer_layers = TRANSFORMER_LAYERS
                config.dropout_rate = DROPOUT_RATE
                config.learning_rate = LEARNING_RATE
                config.patience = PATIENCE
                config.large_error_threshold = LARGE_ERROR_THRESHOLD
                print("âœ… WandB åˆå§‹åŒ–æˆåŠŸï¼ˆç¦»çº¿æ¨¡å¼ï¼‰")
            except Exception as e:
                print(f"âŒ WandB åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                print("â„¹ï¸  ç¨‹åºå°†åœ¨ä¸ä½¿ç”¨ WandB çš„æƒ…å†µä¸‹ç»§ç»­è¿è¡Œ")
                WANDB_ENABLED = False
                wandb_run = None
        else:
            print("â„¹ï¸  WandB å·²è¢«ç¦ç”¨")
        
    # ç¡®ä¿outputsç›®å½•å­˜åœ¨
    os.makedirs('outputs', exist_ok=True)
    
    # 1. æ•°æ®å¤„ç†
    user_map, vocab_size, _, _ = process_data_and_extract_features(FILE_PATH, MAX_ROWS)
    if not user_map: return 

    # 2. æ•°æ®é›†æ„å»º
    X_s, X_d, X_w, X_b, y_st, y_su, valid_users = create_multi_input_dataset(user_map, LOOK_BACK)
    
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›† = 7:1:2
    indices = np.arange(len(y_st))
    # å…ˆåˆ’åˆ†è®­ç»ƒé›†å’Œå‰©ä½™æ•°æ®
    train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)
    # å†ä»å‰©ä½™æ•°æ®ä¸­åˆ’åˆ†éªŒè¯é›†å’Œæµ‹è¯•é›†
    val_idx, test_idx = train_test_split(temp_idx, test_size=2/3, random_state=42)  # 1:2
    
    print(f"æ•°æ®é›†åˆ’åˆ†ï¼šè®­ç»ƒé›† {len(train_idx)}, éªŒè¯é›† {len(val_idx)}, æµ‹è¯•é›† {len(test_idx)}")
    
    # æ„å»º TF Dataset è¾…åŠ©å‡½æ•°
    def make_ds(idx):
        return tf.data.Dataset.from_tensor_slices((
            {
                'input_history': X_s[idx],
                'input_difficulty': X_d[idx],
                'input_word_id': X_w[idx],
                'input_user_bias': X_b[idx]
            },
            {
                'output_steps': y_st[idx],
                'output_success': y_su[idx]
            }
        )).shuffle(20000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
        
    train_ds = make_ds(train_idx)
    # val_ds ç”¨äºæ¨¡å‹è¯„ä¼°ï¼Œä¸éœ€è¦ shuffle
    val_ds = tf.data.Dataset.from_tensor_slices((
            {
                'input_history': X_s[val_idx],
                'input_difficulty': X_d[val_idx],
                'input_word_id': X_w[val_idx],
                'input_user_bias': X_b[val_idx]
            },
            {
                'output_steps': y_st[val_idx],
                'output_success': y_su[val_idx]
            }
        )).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    
    # 3. æ¨¡å‹æ„å»ºä¸è®­ç»ƒ/åŠ è½½
    is_trained = False
    model = None
    
    if os.path.exists(MODEL_SAVE_PATH):
        print(f"Step 3: æ£€æµ‹åˆ°å·²ä¿å­˜æ¨¡å‹ {MODEL_SAVE_PATH}ï¼Œå°è¯•åŠ è½½...")
        try:
            model = tf.keras.models.load_model(MODEL_SAVE_PATH)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè·³è¿‡è®­ç»ƒã€‚")
            is_trained = True
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}. å°†é‡æ–°æ„å»ºå¹¶è®­ç»ƒæ¨¡å‹ã€‚")
            is_trained = False
            
    if not is_trained:
        # æ„å»ºTransformeræ¨¡å‹
        model = build_transformer_model(LOOK_BACK, vocab_size, EMBEDDING_DIM, NUM_HEADS, KEY_DIM, FF_DIM, 
                                       transformer_layers=TRANSFORMER_LAYERS, 
                                       dropout_rate=DROPOUT_RATE, 
                                       learning_rate=LEARNING_RATE)
        
        # è®¾ç½®æ—©åœå›è°ƒ
        early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)
        
        # è®­ç»ƒ
        print(f"Step 4: å¼€å§‹è®­ç»ƒ (Epochs={EPOCHS}, Batch={BATCH_SIZE}, Patience={PATIENCE})...")
        # æ ¹æ®WandBæ˜¯å¦å¯ç”¨å†³å®šå›è°ƒåˆ—è¡¨
        callbacks = [early_stopping]
        if WANDB_ENABLED and wandb_run is not None:
            callbacks.append(WandbCallback(save_model=False))
        
        history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=1,
                           callbacks=callbacks)
        
        # ç»˜åˆ¶å¹¶ä¿å­˜Lossæ›²çº¿
        loss_curve_path = plot_loss_curve(history, 'Transformer', visualization_dir)
        # è®°å½•åˆ°WandB
        if WANDB_ENABLED and wandb_run is not None:
            try:
                wandb.log({'loss_curve': wandb.Image(loss_curve_path)})
            except Exception as e:
                print(f"âŒ WandB æ—¥å¿—è®°å½•å¤±è´¥: {str(e)}")
        
        # è®­ç»ƒå®Œæˆåä¿å­˜
        try:
            model.save(MODEL_SAVE_PATH)
            print(f"\nâœ… æ¨¡å‹å·²æˆåŠŸä¿å­˜åˆ°æ–‡ä»¶å¤¹: {MODEL_SAVE_PATH}")
        except Exception as save_e:
            print(f"\nâŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {save_e}")
            
    # éªŒè¯é›†è¯„ä¼°
    val_inputs = {
        'input_history': X_s[val_idx],
        'input_difficulty': X_d[val_idx],
        'input_word_id': X_w[val_idx],
        'input_user_bias': X_b[val_idx]
    }
    val_labels = {
        'output_steps': y_st[val_idx],
        'output_success': y_su[val_idx]
    }

    val_pred_steps_cont, val_pred_steps_disc, val_pred_success_prob = evaluate_model_and_get_preds(model, val_inputs, val_labels)

    # è®¡ç®— ACC å’Œ AUC
    val_true_wins = val_labels['output_success']
    val_pred_wins = (val_pred_success_prob >= 0.5).astype(int)
    val_acc = accuracy_score(val_true_wins, val_pred_wins)
    
    # è®¡ç®— AUC
    try:
        val_auc = roc_auc_score(val_true_wins, val_pred_success_prob)
    except ValueError:
        print("âš ï¸ AUC è®¡ç®—å¤±è´¥ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ­£è´Ÿæ ·æœ¬ä¸å¹³è¡¡æˆ–ä»…æœ‰å•ä¸€ç±»åˆ«")
        val_auc = 0.5  # é»˜è®¤å€¼
    
    # å°† one-hot æ ‡ç­¾è½¬æ¢ä¸ºç¦»æ•£æ­¥æ•° (1-7)
    val_true_steps = np.argmax(val_labels['output_steps'], axis=1) + 1
    # è®¡ç®—å¤§å‹è¯¯å·®ç‡
    val_large_error_rate = np.mean(np.abs(val_true_steps - val_pred_steps_cont) > LARGE_ERROR_THRESHOLD)
    
    # FIX: å°† one-hot æ ‡ç­¾ (N, 7) è½¬æ¢ä¸ºç¦»æ•£æ­¥æ•° (N,)
    # np.argmax æ‰¾åˆ° 1 çš„ç´¢å¼• (0-6)ï¼Œ+1 è½¬æ¢ä¸ºæ­¥æ•° (1-7)
    val_true_steps_disc = np.argmax(val_labels['output_steps'], axis=1) + 1
    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(
        val_true_steps_disc, val_pred_steps_disc, average='weighted', zero_division=0
    )
    
    # æµ‹è¯•é›†è¯„ä¼°
    test_inputs = {
        'input_history': X_s[test_idx],
        'input_difficulty': X_d[test_idx],
        'input_word_id': X_w[test_idx],
        'input_user_bias': X_b[test_idx]
    }
    test_labels = {
        'output_steps': y_st[test_idx],
        'output_success': y_su[test_idx]
    }

    test_pred_steps_cont, test_pred_steps_disc, test_pred_success_prob = evaluate_model_and_get_preds(model, test_inputs, test_labels)

    # è®¡ç®—æµ‹è¯•é›† ACC å’Œ AUC
    test_true_wins = test_labels['output_success']
    test_pred_wins = (test_pred_success_prob >= 0.5).astype(int)
    test_acc = accuracy_score(test_true_wins, test_pred_wins)
    
    # è®¡ç®—æµ‹è¯•é›† AUC
    try:
        test_auc = roc_auc_score(test_true_wins, test_pred_success_prob)
    except ValueError:
        print("âš ï¸ æµ‹è¯•é›† AUC è®¡ç®—å¤±è´¥ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ­£è´Ÿæ ·æœ¬ä¸å¹³è¡¡æˆ–ä»…æœ‰å•ä¸€ç±»åˆ«")
        test_auc = 0.5  # é»˜è®¤å€¼
    
    # å°† one-hot æ ‡ç­¾è½¬æ¢ä¸ºç¦»æ•£æ­¥æ•° (1-7)
    test_true_steps = np.argmax(test_labels['output_steps'], axis=1) + 1
    # è®¡ç®—å¤§å‹è¯¯å·®ç‡
    test_large_error_rate = np.mean(np.abs(test_true_steps - test_pred_steps_cont) > LARGE_ERROR_THRESHOLD)
    
    # FIX: å°† one-hot æ ‡ç­¾ (N, 7) è½¬æ¢ä¸ºç¦»æ•£æ­¥æ•° (N,)
    test_true_steps_disc = np.argmax(test_labels['output_steps'], axis=1) + 1
    test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(
        test_true_steps_disc, test_pred_steps_disc, average='weighted', zero_division=0
    )
    # è®°å½•éªŒè¯æŒ‡æ ‡åˆ° WandB
    if WANDB_ENABLED and wandb_run is not None:
        try:
            wandb.log({
                "val_accuracy": val_acc,
                "val_auc": val_auc,
                "val_large_error_rate": val_large_error_rate,
                "val_precision": val_precision,
                "val_recall": val_recall,
                "val_f1": val_f1,
                "test_precision": test_precision,
                "test_recall": test_recall,
                "test_f1": test_f1,
                "test_accuracy": test_acc,
                "test_auc": test_auc,
                "test_large_error_rate": test_large_error_rate
            })
        except Exception as e:
            print(f"âŒ WandB æ—¥å¿—è®°å½•å¤±è´¥: {str(e)}")
    
    # 5. éªŒè¯æŠ½æ ·æŠ¥å‘Š (ä½¿ç”¨æ‰¹é‡ç»“æœè®¡ç®—çš„ MAE/ACC)
    
    # ç”ŸæˆæŠ¥å‘Šçš„å¤´éƒ¨å’Œæ±‡æ€»æŒ‡æ ‡
    report = f"""
========================================
  Transformeræ¨¡å‹éªŒè¯å’Œæµ‹è¯•æŠ¥å‘Š
========================================
---- éªŒè¯é›†æŒ‡æ ‡ ----
1. èƒœè´Ÿé¢„æµ‹å‡†ç¡®ç‡        : {val_acc:.3%}
2. ROCæ›²çº¿ä¸‹é¢ç§¯ (AUC)   : {val_auc:.4f}
3. å¤§å‹è¯¯å·®ç‡ (>{LARGE_ERROR_THRESHOLD}æ­¥)  : {val_large_error_rate:.3%}
4. ç²¾ç¡®ç‡ (Precision)    : {val_precision:.4f}
5. å¬å›ç‡ (Recall)       : {val_recall:.4f}
6. F1å€¼ (F1-Score)       : {val_f1:.4f}

---- æµ‹è¯•é›†æŒ‡æ ‡ ----
1. èƒœè´Ÿé¢„æµ‹å‡†ç¡®ç‡        : {test_acc:.3%}
2. ROCæ›²çº¿ä¸‹é¢ç§¯ (AUC)   : {test_auc:.4f}
3. å¤§å‹è¯¯å·®ç‡ (>{LARGE_ERROR_THRESHOLD}æ­¥)  : {test_large_error_rate:.3%}
4. ç²¾ç¡®ç‡ (Precision)    : {test_precision:.4f}
5. å¬å›ç‡ (Recall)       : {test_recall:.4f}
6. F1å€¼ (F1-Score)       : {test_f1:.4f}
========================================
"""
    # å†™å…¥å…¨å±€æŠ¥å‘Š
    with open("outputs/transformer_output.txt", "w", encoding="utf-8") as f:
        f.write(report)
    print(report)
    
    # è°ƒç”¨ perform_validation è¿›è¡ŒæŠ½æ ·æŠ¥å‘Šï¼ˆä½¿ç”¨æµ‹è¯•é›†é¢„æµ‹ç»“æœï¼‰
    # ç”±äºå·²åˆ’åˆ†æµ‹è¯•é›†ï¼Œä¸å†éœ€è¦VALIDATION_SAMPLE_SIZEå‚æ•°
    perform_validation(model, user_map, valid_users, LOOK_BACK, 
                       min(10000, len(test_idx)), LARGE_ERROR_THRESHOLD, test_pred_steps_cont)
    
    # ç»˜åˆ¶å¹¶ä¿å­˜é¢„æµ‹è¶‹åŠ¿å›¾ï¼ˆä½¿ç”¨æµ‹è¯•é›†ç»“æœï¼‰
    # FIX: ä½¿ç”¨å·²è½¬æ¢ä¸ºå•å€¼å½¢å¼ (N,) çš„çœŸå®æ­¥æ•° test_true_steps_disc è¿›è¡Œç»˜å›¾
    prediction_trend_path = plot_prediction_trends(
        test_true_steps_disc, test_pred_steps_cont, test_pred_steps_disc, 'Transformer', visualization_dir
    )
    # è®°å½•åˆ°WandB
    if WANDB_ENABLED and wandb_run is not None:
        try:
            wandb.log({'prediction_trends': wandb.Image(prediction_trend_path)})
        except Exception as e:
            print(f"âŒ WandB æ—¥å¿—è®°å½•å¤±è´¥: {str(e)}")
    
    # ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µï¼ˆä½¿ç”¨æµ‹è¯•é›†ç»“æœï¼‰
    confusion_matrix_path = plot_confusion_matrix(
        test_labels['output_steps'], test_pred_steps_disc, 'Transformer', visualization_dir
    )
    # è®°å½•åˆ°WandB
    if WANDB_ENABLED and wandb_run is not None:
        try:
            wandb.log({'confusion_matrix': wandb.Image(confusion_matrix_path)})
        except Exception as e:
            print(f"âŒ WandB æ—¥å¿—è®°å½•å¤±è´¥: {str(e)}")
            # è¿›è¡Œé¢„æµ‹
    pred_steps_cont, pred_steps_disc, pred_success = predict_with_model(model, user_map, user_id, LOOK_BACK)
    if pred_steps_cont is not None:
        print(f"\nç”¨æˆ· {user_id} çš„é¢„æµ‹ç»“æœ:")
        print(f"è¿ç»­é¢„æµ‹æ­¥æ•°: {pred_steps_cont:.2f}")
        print(f"ç¦»æ•£é¢„æµ‹æ­¥æ•° (1-7): {pred_steps_disc}")
        print(f"æˆåŠŸæ¦‚ç‡: {pred_success:.2%}")
        print(f"é¢„æµ‹ç»“æœ: {'æˆåŠŸ' if pred_steps_disc <= 6 else 'å¤±è´¥'}")
        
    else:
        print(f"âŒ æœªçŸ¥æ¨¡å¼: {mode}ï¼Œè¯·ä½¿ç”¨ 'train' æˆ– 'predict'")

if __name__ == "__main__":
    # ç¡®ä¿å¿…è¦çš„æ–‡ä»¶å¤¹å­˜åœ¨
    os.makedirs('wandb', exist_ok=True)
    os.makedirs('visualization', exist_ok=True)
    os.makedirs('outputs', exist_ok=True)
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°
    import sys
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        user_id = sys.argv[2] if len(sys.argv) > 2 else None
        main(mode, user_id)
    else:
        main()