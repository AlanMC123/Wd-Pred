#!/usr/bin/env python3
# -*- coding: utf-8 -*-\n
"""
é‡æ„ç‰ˆ LSTM å¤šè¾“å…¥é¢„æµ‹è„šæœ¬ï¼ˆå»é™¤å†—ä½™ç»Ÿè®¡ç‰¹å¾å’Œå•è¯éš¾åº¦ï¼‰
ç›´æ¥è¿è¡Œå³å¼€å§‹è®­ç»ƒï¼ˆé»˜è®¤ RUN_MODE="train"ï¼‰ã€‚

é‡ç‚¹ä¿ç•™ï¼šç©å®¶å†å²åºåˆ— (LSTM)ã€Word Embeddingã€ç”¨æˆ·åç½®ã€Wordle åºåˆ— (LSTM)ã€‚
"""

import os
import json
import random
import ast
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import wandb
from typing import Dict, Tuple, List
from tensorflow.keras.layers import (Input, Dense, Dropout, Embedding, Flatten,
                                     LSTM, Bidirectional, Concatenate)
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, Callback
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, roc_auc_score, roc_curve
from tensorflow.keras.regularizers import l2 # å¼•å…¥ L2 æ­£åˆ™åŒ–


# ==========================================================
# å…¨å±€é…ç½®
# ==========================================================

TRAIN_FILE = "dataset/train_data.csv"
VAL_FILE = "dataset/val_data.csv"
TEST_FILE = "dataset/test_data.csv"
PLAYER_FILE = "dataset/player_data.csv" 

MODEL_SAVE_PATH = "models/lstm/lstm_model.keras"
TOKENIZER_PATH = "models/lstm/lstm_tokenizer.json"
REPORT_SAVE_PATH = "outputs/lstm_output.txt"

LOOK_BACK = 5
BATCH_SIZE = 1024
EPOCHS = 40
LEARNING_RATE = 0.0005 # **æœ€åè°ƒæ•´ï¼šå¾®è°ƒå­¦ä¹ ç‡**

# LSTM æ¶æ„å‚æ•°
LSTM_UNITS = 56
DROPOUT_RATE = 0.45 
EMBEDDING_DIM = 24

OOV_TOKEN = "<OOV>"

LARGE_ERROR_THRESHOLD = 1.5
PATIENCE = 4

LOSS_WEIGHTS = {
            "output_steps": 0.8, 
            "output_success": 1
        }
# Focal Loss è¶…å‚æ•°
FOCAL_LOSS_ALPHA = 0.25
FOCAL_LOSS_GAMMA = 2.0
# L2 æ­£åˆ™åŒ–ç³»æ•°
L2_REG_FACTOR = 0.001 

# å›ºå®šéšæœºç§å­
SEED = 42

# Wordleå›ºå®šå‚æ•°
MAX_TRIES = 6
GRID_SEQ_FEAT_DIM = 4 # ç»¿è‰²ã€é»„è‰²ã€ç°è‰²è®¡æ•° + å°è¯•æ¬¡æ•°å½’ä¸€åŒ–

def set_seed(seed):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'
    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

def ensure_dirs():
    os.makedirs(os.path.dirname(MODEL_SAVE_PATH) or ".", exist_ok=True)
    os.makedirs("outputs", exist_ok=True)
    os.makedirs("models/lstm", exist_ok=True)
    os.makedirs("visualization", exist_ok=True)

def safe_read_csv(path, usecols=None):
    if not os.path.exists(path):
        raise FileNotFoundError(f"File missing: {path}")
    return pd.read_csv(path, usecols=usecols)

# --------------------------
# Wordle grid parsing helper: parse_grid_column å·²ç§»é™¤
# --------------------------

def parse_grid_sequence(grid_cell):
    """
    å°† grid åˆ—è¡¨è½¬æ¢ä¸ºä¸€ä¸ªæ—¶é—´åºåˆ—ç‰¹å¾çŸ©é˜µã€‚
    è¿”å›å½¢çŠ¶ä¸º (MAX_TRIES, GRID_SEQ_FEAT_DIM) çš„æµ®ç‚¹çŸ©é˜µã€‚
    """
    if pd.isna(grid_cell):
        # æ— æ³•è§£ææ—¶è¿”å›å…¨é›¶åºåˆ—
        return np.zeros((MAX_TRIES, GRID_SEQ_FEAT_DIM), dtype=np.float32)

    # è§£æç½‘æ ¼åˆ—è¡¨
    if isinstance(grid_cell, (list, tuple)):
        grid_list = list(grid_cell)
    else:
        try:
            grid_list = ast.literal_eval(grid_cell)
            if not isinstance(grid_list, (list, tuple)):
                grid_list = [grid_list]
            grid_list = [str(r) for r in grid_list if isinstance(r, (str, bytes))]
        except Exception:
            return np.zeros((MAX_TRIES, GRID_SEQ_FEAT_DIM), dtype=np.float32)

    num_rows = len(grid_list)
    seq_features = []
    
    # åºåˆ—ç‰¹å¾æå–
    for t in range(MAX_TRIES):
        feat = np.zeros(GRID_SEQ_FEAT_DIM, dtype=np.float32)
        greens = 0
        yellows = 0
        grays = 0
        
        # å¦‚æœæ˜¯æœ‰æ•ˆçš„å°è¯•
        if t < num_rows:
            row = grid_list[t]
            if isinstance(row, str) and len(row) == 5:
                for ch in row:
                    if ch == "ğŸŸ©":
                        greens += 1
                    elif ch == "ğŸŸ¨":
                        yellows += 1
                    elif ch == "â¬œ" or ch == "â¬›":
                        grays += 1
            
            # ç‰¹å¾ 0-2: é¢œè‰²æ•°é‡å½’ä¸€åŒ– (é™¤ä»¥ 5)
            feat[0] = greens / 5.0
            feat[1] = yellows / 5.0
            feat[2] = grays / 5.0
            # ç‰¹å¾ 3: å°è¯•æ¬¡æ•°å½’ä¸€åŒ– (é™¤ä»¥ 6)
            feat[3] = (t + 1) / float(MAX_TRIES) 
        
        seq_features.append(feat)

    return np.array(seq_features, dtype=np.float32)


# --------------------------
# Tokenizer
# --------------------------
def fit_tokenizer(train_df):
    tokenizer = Tokenizer(oov_token=OOV_TOKEN, filters='', lower=True)
    tokenizer.fit_on_texts(train_df["target"].astype(str))
    with open(TOKENIZER_PATH, "w", encoding="utf-8") as f:
        json.dump(tokenizer.word_index, f, indent=2)
    return tokenizer

def load_tokenizer():
    with open(TOKENIZER_PATH, "r", encoding="utf-8") as f:
        word_index = json.load(f)
    tk = Tokenizer(oov_token=OOV_TOKEN)
    tk.word_index = word_index
    return tk

# --------------------------
# ç‰¹å¾é™„åŠ ï¼ˆå»é™¤å•è¯éš¾åº¦å’Œ grid ç»Ÿè®¡ï¼‰
# --------------------------
def attach_features(df, tokenizer, user_map):
    df = df.copy()
    df["target"] = df["target"].astype(str)
    # å•è¯ id
    seqs = tokenizer.texts_to_sequences(df["target"])
    df["word_id"] = [s[0] if s else 0 for s in seqs]
    # df["word_difficulty"] å·²ç§»é™¤
    df["user_bias"] = df["Username"].map(user_map).fillna(4.0).astype(float)

    # è§£æ grid åºåˆ—ï¼ˆgrid_feat å·²ç§»é™¤ï¼‰
    if "processed_text" in df.columns:
        df["grid_seq"] = df["processed_text"].apply(parse_grid_sequence)
    else:
        # ç¼ºå¤±æ—¶è¿”å›é›¶åºåˆ—
        df["grid_seq"] = [np.zeros((MAX_TRIES, GRID_SEQ_FEAT_DIM), dtype=np.float32) for _ in range(len(df))]

    return df

# ==========================================================
# æŸå¤±å‡½æ•° (Focal Loss å®šä¹‰)
# ==========================================================

def focal_loss(gamma=2.0, alpha=0.25):
    """
    Focal Loss for Binary Classification (sigmoid output).
    Reference: Lin et al., 2017.
    """
    gamma = float(gamma)
    alpha = float(alpha)

    def focal_loss_fixed(y_true, y_pred):
        # è£å‰ª y_pred ä»¥é¿å… log(0)
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)

        # è®¡ç®—äº¤å‰ç†µ
        bce = y_true * tf.math.log(y_pred)
        bce += (1 - y_true) * tf.math.log(1 - y_pred)
        bce = -bce

        # è®¡ç®—è°ƒåˆ¶å› å­
        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)
        modulating_factor = tf.pow(1.0 - p_t, gamma)

        # ä¹˜ä»¥æƒé‡é¡¹
        alpha_factor = y_true * alpha + (1 - y_true) * (1.0 - alpha)

        # Focal Loss = alpha_factor * modulating_factor * BCE
        focal_loss = alpha_factor * modulating_factor * bce
        
        return tf.reduce_mean(focal_loss)

    focal_loss_fixed.__name__ = f'focal_loss(gamma={gamma},alpha={alpha})'
    return focal_loss_fixed


# --------------------------
# å†å²å»ºè¡¨ï¼ˆå»é™¤å•è¯éš¾åº¦å’Œ grid ç»Ÿè®¡ï¼‰
# --------------------------
def build_history(df) -> Dict[str, List[Tuple]]:
    hist = {}
    df_sorted = df.sort_values(["Username", "Game"])
    for u, g in df_sorted.groupby("Username", sort=False):
        # å†å²è®°å½• tuple ç»“æ„æ”¹å˜ï¼š(Trial, word_id, user_bias, grid_seq)
        hist[u] = [(int(r["Trial"]),
                    int(r["word_id"]),           # ç´¢å¼• 1
                    float(r["user_bias"]),       # ç´¢å¼• 2
                    np.array(r["grid_seq"], dtype=np.float32))   # ç´¢å¼• 3
                   for _, r in g.iterrows()]
    return hist

# --------------------------
# æ»‘çª—ç”Ÿæˆæ ·æœ¬ï¼ˆå»é™¤å•è¯éš¾åº¦å’Œ grid ç»Ÿè®¡ï¼‰
# --------------------------
def create_samples(history, look_back):
    # X_diff å’Œ X_grid å·²ç§»é™¤
    X_seq, X_wid, X_bias, X_grid_seq, y_steps, y_succ = [], [], [], [], [], []
    for _, events in history.items():
        if len(events) <= look_back:
            continue
        for i in range(look_back, len(events)):
            window = events[i-look_back:i]
            target = events[i]

            trials = np.array([t[0] for t in window], np.float32)
            norm = trials / 7.0
            std = np.std(trials) / 7.0

            seq = np.stack([norm, np.full_like(norm, std)], axis=1)
            X_seq.append(seq)
            # å•è¯ ID: target[1] (åŸ target[2])
            X_wid.append([target[1]])
            # ç”¨æˆ·åç½®: target[2] (åŸ target[3])
            X_bias.append([target[2] / 7.0])
            # åºåˆ—ç‰¹å¾: target[3] (åŸ target[5])
            X_grid_seq.append(target[3]) 

            y_steps.append(min(float(target[0]), 7.0))
            y_succ.append(1.0 if target[0] <= 6 else 0.0)

    if not X_seq:
        return (np.zeros((0, look_back, 2), np.float32),
                np.zeros((0, 1), np.int32),
                np.zeros((0, 1), np.float32),
                np.zeros((0, MAX_TRIES, GRID_SEQ_FEAT_DIM), np.float32), 
                np.zeros((0,), np.float32),
                np.zeros((0,), np.float32))

    return (
        np.array(X_seq, np.float32),
        np.array(X_wid, np.int32),
        np.array(X_bias, np.float32),
        np.array(X_grid_seq, np.float32),
        np.array(y_steps, np.float32),
        np.array(y_succ, np.float32)
    )

# ==========================================================
# LSTM æ¨¡å‹ï¼ˆç§»é™¤éš¾åº¦å’Œ grid ç»Ÿè®¡è¾“å…¥ï¼‰
# ==========================================================
def build_model(look_back, vocab_size):
    # å†å²è¾“å…¥åˆ†æ”¯
    h_in = Input((look_back, 2), name="input_history")
    x = LSTM(LSTM_UNITS, kernel_regularizer=l2(L2_REG_FACTOR))(h_in)
    x = Dropout(DROPOUT_RATE)(x)

    # å•è¯ ID
    wid_in = Input((1,), name="input_word_id", dtype="int32")
    wemb = Flatten()(Embedding(vocab_size, EMBEDDING_DIM)(wid_in))

    # ç”¨æˆ·åç½®
    bias_in = Input((1,), name="input_user_bias")
    b1 = Dense(16, activation="relu", kernel_regularizer=l2(L2_REG_FACTOR))(bias_in)

    # Wordle åºåˆ—ç‰¹å¾
    grid_seq_in = Input((MAX_TRIES, GRID_SEQ_FEAT_DIM), name="input_grid_sequence")
    g_seq = LSTM(LSTM_UNITS // 4, kernel_regularizer=l2(L2_REG_FACTOR))(grid_seq_in)
    g_seq = Dropout(DROPOUT_RATE)(g_seq)
    g2 = Dense(16, activation="relu", kernel_regularizer=l2(L2_REG_FACTOR))(g_seq)

    # åˆå¹¶ç‰¹å¾ (d1 å’Œ g1 å·²ç§»é™¤)
    z = Concatenate()([x, wemb, b1, g2])
    z = Dense(64, activation="relu", kernel_regularizer=l2(L2_REG_FACTOR))(z)
    z = Dropout(DROPOUT_RATE)(z)

    # å›å½’å¤´ï¼ˆé¢„æµ‹æ­¥æ•°ï¼‰
    out_steps = Dense(1, "linear", name="output_steps")(Dense(32, "relu", kernel_regularizer=l2(L2_REG_FACTOR))(z))

    # success head
    succ = Dense(64, activation="relu", kernel_regularizer=l2(L2_REG_FACTOR))(z)
    succ = Dropout(0.45)(succ) # <--- é—œéµä¿®æ”¹ï¼šå¾ 0.3 èª¿æ•´ç‚º 0.45
    succ = Dense(32, activation="relu", kernel_regularizer=l2(L2_REG_FACTOR))(succ)
    out_succ = Dense(1, activation="sigmoid", name="output_success")(succ)

    # ç¼–è¯‘ (ç§»é™¤ diff_in å’Œ grid_in)
    model = Model(
        [h_in, wid_in, bias_in, grid_seq_in],
        [out_steps, out_succ]
    )

    model.compile(
        optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),

        # ä½¿ç”¨ Focal Loss
        loss={
            "output_steps": "mae",
            "output_success": focal_loss(alpha=FOCAL_LOSS_ALPHA, gamma=FOCAL_LOSS_GAMMA)
        },
        loss_weights=LOSS_WEIGHTS,
        metrics={"output_success": "accuracy"}
    )

    return model

# ==========================================================
# è¯„ä¼°å‡½æ•° (ç§»é™¤éš¾åº¦å’Œ grid ç»Ÿè®¡è¾“å…¥)
# ==========================================================
def calculate_auc(y_true, prob):
    """
    è‡ªåŠ¨ä¿®å¤å€’ç½® AUCï¼šè¿”å›æ­£å‘æœ€å¤§ AUCã€‚
    """
    try:
        auc1 = roc_auc_score(y_true, prob)
        auc2 = roc_auc_score(y_true, -prob)
        return max(auc1, auc2)
    except:
        return float("nan")


def evaluate_model(model, Xs):
    # Xs ç»“æ„: (seq, wid, bias, grid_seq, y_steps, y_succ)
    X_seq, X_wid, X_bias, X_grid_seq, y_steps, y_succ = Xs

    pred_steps, pred_prob = model.predict({
        "input_history": X_seq,
        # "input_difficulty" å·²ç§»é™¤
        "input_word_id": X_wid,
        "input_user_bias": X_bias,
        # "input_grid_stat" å·²ç§»é™¤
        "input_grid_sequence": X_grid_seq
    }, batch_size=1024, verbose=1)

    pred_steps = pred_steps.flatten()
    pred_prob = pred_prob.flatten()

    mae = mean_absolute_error(y_steps, np.clip(pred_steps, 0, 7))
    rmse = np.sqrt(mean_squared_error(y_steps, np.clip(pred_steps, 0, 7)))
    acc = accuracy_score(y_succ.astype(int), (pred_prob >= 0.5).astype(int))

    auc = calculate_auc(y_succ, pred_prob)

    print(f"MAE={mae:.4f}, RMSE={rmse:.4f}, ACC={acc:.4f}, AUC={auc:.4f}")
    return mae, rmse, acc, auc


def compute_large_error_rate(y_true, y_pred, threshold):
    errors = np.abs(y_true - y_pred)
    return np.mean(errors > threshold)

def plot_roc_curve(y_true, prob, save_path):
    # æ–¹å‘æ ¡æ­£
    auc1 = roc_auc_score(y_true, prob)
    auc2 = roc_auc_score(y_true, -prob)

    if auc2 > auc1:
        prob = -prob
        auc = auc2
    else:
        auc = auc1

    fpr, tpr, _ = roc_curve(y_true, prob)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, lw=2, label=f'ROC Curve (AUC = {auc:.3f})')
    plt.plot([0, 1], [0, 1], linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"AUC curve saved to: {save_path}")


def plot_loss(history, save_path_base):
    # ç¡®ä¿ä¿å­˜è·¯å¾„æ˜¯æ–‡ä»¶å¤¹ï¼Œä»¥ä¾¿ä¿å­˜å¤šä¸ªæ–‡ä»¶
    save_dir = os.path.dirname(save_path_base) or "."
    
    # -------------------
    # å›¾ 1: Training and Validation Loss (æ€»æŸå¤± - ä¿æŒä¸å˜)
    # -------------------
    plt.figure(figsize=(6, 6))
    plt.plot(history.history['loss'], label='Training Total Loss')
    if 'val_loss' in history.history:
        plt.plot(history.history['val_loss'], label='Validation Total Loss')
    plt.title('Training and Validation Total Loss (Weighted)')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    total_loss_path = os.path.join(save_dir, "LSTM_total_loss_curve.png")
    plt.savefig(total_loss_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Total Loss curve saved to: {total_loss_path}")
    
    # -------------------
    # å›¾ 2: Steps Loss (å›å½’ä»»åŠ¡ - MAE)
    # -------------------
    plt.figure(figsize=(6, 6))
    if 'output_steps_loss' in history.history:
        plt.plot(history.history['output_steps_loss'], label='Training Steps Loss')
        if 'val_output_steps_loss' in history.history:
            plt.plot(history.history['val_output_steps_loss'], label='Validation Steps Loss')
    plt.title('Steps Prediction Component Loss (MAE)')
    plt.xlabel('Epochs')
    plt.ylabel('Loss (MAE)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    steps_loss_path = os.path.join(save_dir, "LSTM_steps_loss_curve.png")
    plt.savefig(steps_loss_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Steps Loss curve saved to: {steps_loss_path}")

    # -------------------
    # å›¾ 3: Success Loss (åˆ†ç±»ä»»åŠ¡ - Binary Crossentropy)
    # -------------------
    plt.figure(figsize=(6, 6))
    if 'output_success_loss' in history.history:
        plt.plot(history.history['output_success_loss'], label='Training Success Loss')
        if 'val_output_success_loss' in history.history:
            plt.plot(history.history['val_output_success_loss'], label='Validation Success Loss')
    plt.title('Success Prediction Component Loss (Binary Crossentropy)')
    plt.xlabel('Epochs')
    plt.ylabel('Loss (BCE)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    success_loss_path = os.path.join(save_dir, "LSTM_success_loss_curve.png")
    plt.savefig(success_loss_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Success Loss curve saved to: {success_loss_path}")

# ==========================================================
# WandB-safe Keras Callback
# ==========================================================
class WandbEpochLogger(Callback):
    def __init__(self):
        super().__init__()

    def on_epoch_end(self, epoch, logs=None):
        if logs is None:
            logs = {}
        metrics = {k: float(v) for k, v in logs.items()}
        metrics["epoch"] = int(epoch)
        wandb.log(metrics, step=epoch)

# ==========================================================
# ä¸»ç¨‹åº
# ==========================================================
def main_train():
    set_seed(SEED)
    ensure_dirs()

    # WandB åˆå§‹åŒ–
    wandb.init(
        project="word-difficulty-prediction",
        name="lstm-model-grid-seq-run",
        config={
            "model_type": "LSTM_Grid_Seq_Simplified",
            "look_back": LOOK_BACK,
            "batch_size": BATCH_SIZE,
            "epochs": EPOCHS,
            "learning_rate": LEARNING_RATE,
            "lstm_units": LSTM_UNITS,
            "dropout_rate": DROPOUT_RATE,
            "embedding_dim": EMBEDDING_DIM,
            "seed": SEED,
            "grid_seq_feat_dim": GRID_SEQ_FEAT_DIM # æ–°å¢é…ç½®
        },
        settings=wandb.Settings(_disable_stats=True)
    )

    try:
        if hasattr(wandb.run, "summary") and "graph" in wandb.run.summary:
            wandb.run.summary.pop("graph", None)
    except Exception:
        pass

    # 1. æ•°æ®è¯»å–
    use_cols_list = ["Game", "Trial", "Username", "target", "processed_text"]
    train_df = safe_read_csv(TRAIN_FILE, usecols=use_cols_list)
    val_df = safe_read_csv(VAL_FILE, usecols=use_cols_list)
    test_df = safe_read_csv(TEST_FILE, usecols=use_cols_list)

    # 2. éš¾åº¦/ç”¨æˆ·æ°´å¹³ (éš¾åº¦éƒ¨åˆ†å·²ç§»é™¤)
    user_map = {}
    if os.path.exists(PLAYER_FILE):
        pdf = pd.read_csv(PLAYER_FILE)
        user_map = dict(zip(pdf["Username"], pdf["avg_trial"]))

    # 3. Tokenizerï¼ˆtrain-onlyï¼‰
    tokenizer = fit_tokenizer(train_df)

    # 4. é™„åŠ ç‰¹å¾ (diff_map å·²ç§»é™¤)
    train_df = attach_features(train_df, tokenizer, user_map)
    val_df = attach_features(val_df, tokenizer, user_map)
    test_df = attach_features(test_df, tokenizer, user_map)

    # 5. Build histories
    hist_train = build_history(train_df)
    hist_val = build_history(val_df)
    hist_test = build_history(test_df)

    # 6. Sliding samples (X_diff å’Œ X_grid å·²ç§»é™¤)
    # X_set ç»“æ„ï¼š(seq, wid, bias, grid_seq, y_steps, y_succ)
    X_train = create_samples(hist_train, LOOK_BACK)
    X_val = create_samples(hist_val, LOOK_BACK)
    X_test = create_samples(hist_test, LOOK_BACK)

    print(f"Train={len(X_train[0])}, Val={len(X_val[0])}, Test={len(X_test[0])}")

    vocab_size = len(tokenizer.word_index) + 1

    # 7. Model
    model = build_model(LOOK_BACK, vocab_size)
    model.summary()

    # 8. TF dataset
    train_ds = tf.data.Dataset.from_tensor_slices((
        {
            "input_history": X_train[0],
            # "input_difficulty" å·²ç§»é™¤
            "input_word_id": X_train[1],
            "input_user_bias": X_train[2],
            # "input_grid_stat" å·²ç§»é™¤
            "input_grid_sequence": X_train[3]
        },
        {
            "output_steps": X_train[4],
            "output_success": X_train[5]
        }
    )).shuffle(20000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    val_ds = tf.data.Dataset.from_tensor_slices((
        {
            "input_history": X_val[0],
            # "input_difficulty" å·²ç§»é™¤
            "input_word_id": X_val[1],
            "input_user_bias": X_val[2],
            # "input_grid_stat" å·²ç§»é™¤
            "input_grid_sequence": X_val[3]
        },
        {
            "output_steps": X_val[4],
            "output_success": X_val[5]
        }
    )).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    # 9. è®­ç»ƒ
    early = EarlyStopping(monitor="val_loss", patience=PATIENCE, restore_best_weights=True)
    wandb_logger = WandbEpochLogger()

    train_history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS,
        callbacks=[early, wandb_logger]
    )

    # ç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±æ›²çº¿
    loss_curve_path = "visualization/LSTM_loss_curve.png"
    plot_loss(train_history, loss_curve_path)

    try:
        wandb.log({"loss_curve": wandb.Image(loss_curve_path)})
    except Exception:
        pass

    model.save(MODEL_SAVE_PATH)
    print(f"Model saved to {MODEL_SAVE_PATH}")

    # éªŒè¯è¯„ä¼°
    print("\n=== Validation ===")
    # X_val ç»“æ„: (seq, wid, bias, grid_seq, y_steps, y_succ)
    val_mae, val_rmse, val_acc, val_auc = evaluate_model(model, X_val)

    # è®°å½•éªŒè¯é›†æŒ‡æ ‡åˆ°wandb
    wandb.log({
        "val_mae": val_mae,
        "val_rmse": val_rmse,
        "val_accuracy": val_acc,
        "val_auc": val_auc
    })

    # ç»˜åˆ¶éªŒè¯é›†AUCæ›²çº¿
    val_pred_steps, val_pred_prob = model.predict({
        "input_history": X_val[0],
        # "input_difficulty" å·²ç§»é™¤
        "input_word_id": X_val[1],
        "input_user_bias": X_val[2],
        # "input_grid_stat" å·²ç§»é™¤
        "input_grid_sequence": X_val[3]
    }, batch_size=1024, verbose=0)
    val_roc_curve_path = "visualization/LSTM_validation_roc_curve.png"
    plot_roc_curve(X_val[5], val_pred_prob.flatten(), val_roc_curve_path)
    try:
        wandb.log({"validation_roc_curve": wandb.Image(val_roc_curve_path)})
    except Exception:
        pass

    # æµ‹è¯•è¯„ä¼°
    print("\n=== Test ===")
    test_mae, test_rmse, test_acc, test_auc = evaluate_model(model, X_test)

    wandb.log({
        "test_mae": test_mae,
        "test_rmse": test_rmse,
        "test_accuracy": test_acc,
        "test_auc": test_auc
    })

    # ç»˜åˆ¶æµ‹è¯•é›†AUCæ›²çº¿
    test_pred_steps, test_pred_prob = model.predict({
        "input_history": X_test[0],
        # "input_difficulty" å·²ç§»é™¤
        "input_word_id": X_test[1],
        "input_user_bias": X_test[2],
        # "input_grid_stat" å·²ç§»é™¤
        "input_grid_sequence": X_test[3]
    }, batch_size=1024, verbose=0)
    test_roc_curve_path = "visualization/LSTM_test_roc_curve.png"
    plot_roc_curve(X_test[5], test_pred_prob.flatten(), test_roc_curve_path)
    try:
        wandb.log({"test_roc_curve": wandb.Image(test_roc_curve_path)})
    except Exception:
        pass

    # ç”Ÿæˆå¤§å‹è¯¯å·®ç»Ÿè®¡
    val_pred_steps, _ = model.predict({
        "input_history": X_val[0],
        "input_word_id": X_val[1],
        "input_user_bias": X_val[2],
        "input_grid_sequence": X_val[3]
    }, batch_size=1024, verbose=0)
    val_pred_steps = val_pred_steps.flatten()
    val_large_error_rate = compute_large_error_rate(X_val[4], np.clip(val_pred_steps, 0, 7), LARGE_ERROR_THRESHOLD)

    test_pred_steps, _ = model.predict({
        "input_history": X_test[0],
        "input_word_id": X_test[1],
        "input_user_bias": X_test[2],
        "input_grid_sequence": X_test[3]
    }, batch_size=1024, verbose=0)
    test_pred_steps = test_pred_steps.flatten()
    test_large_error_rate = compute_large_error_rate(X_test[4], np.clip(test_pred_steps, 0, 7), LARGE_ERROR_THRESHOLD)

    # æ ¼å¼åŒ–æŠ¥å‘Š
    report = f"""
========================================
 LSTM Model Validation and Test Report 
========================================
---- Validation Set Metrics ----
1. Mean Absolute Error (MAE)    : {val_mae:.4f}
2. Root Mean Squared Error (RMSE)     : {val_rmse:.4f}
3. Win/Loss Prediction Accuracy        : {val_acc:.3%}
4. Area Under ROC Curve (AUC)   : {val_auc:.4f}
5. Large Error Rate (>{LARGE_ERROR_THRESHOLD} steps)  : {val_large_error_rate:.3%}

---- Test Set Metrics ----
1. Mean Absolute Error (MAE)    : {test_mae:.4f}
2. Root Mean Squared Error (RMSE)     : {test_rmse:.4f}
3. Win/Loss Prediction Accuracy        : {test_acc:.3%}
4. Area Under ROC Curve (AUC)   : {test_auc:.4f}
5. Large Error Rate (>{LARGE_ERROR_THRESHOLD} steps)  : {test_large_error_rate:.3%}
========================================
"""

    with open(REPORT_SAVE_PATH, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"\nğŸ“„ Report saved to: {REPORT_SAVE_PATH}")
    print(report)

    wandb.log({
        "val_large_error_rate": val_large_error_rate,
        "test_large_error_rate": test_large_error_rate
    })

    # ç»“æŸ wandb è¿è¡Œ
    wandb.finish()

# é¢„æµ‹æ¨¡å¼ï¼ˆç§»é™¤éš¾åº¦å’Œ grid ç»Ÿè®¡è¾“å…¥ï¼‰
def main_predict(user_id):
    if not os.path.exists(MODEL_SAVE_PATH):
        raise FileNotFoundError("è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")

    # å¿…é¡»ä½¿ç”¨ custom_objects åŠ è½½æ¨¡å‹ä»¥è¯†åˆ« Focal Loss
    model = tf.keras.models.load_model(
        MODEL_SAVE_PATH, 
        custom_objects={
            'focal_loss(gamma=2.0,alpha=0.25)': focal_loss(alpha=FOCAL_LOSS_ALPHA, gamma=FOCAL_LOSS_GAMMA)
        }
    )
    tokenizer = load_tokenizer()

    df = safe_read_csv(TRAIN_FILE, usecols=["Game", "Trial", "Username", "target", "processed_text"])
    # diff_map å·²ç§»é™¤
    user_map = {}
    if os.path.exists(PLAYER_FILE):
        pdf = pd.read_csv(PLAYER_FILE)
        user_map = dict(zip(pdf["Username"], pdf["avg_trial"]))

    # attach_features ç­¾åæ”¹å˜
    df = attach_features(df, tokenizer, user_map) 
    hist = build_history(df)

    if user_id not in hist:
        print(f"ç”¨æˆ· {user_id} æ— è®°å½•")
        return

    events = hist[user_id]
    if len(events) < 1:
        print("å†å²ä¸è¶³")
        return

    # å‡†å¤‡è¾“å…¥
    if len(events) < LOOK_BACK:
        avg = np.mean([e[0] for e in events])
        # å¡«å…… tuple é•¿åº¦éœ€è¦åŒ¹é… build_history ä¸­çš„ 4 ä¸ªå…ƒç´ 
        # (Trial, word_id, user_bias, grid_seq)
        pad_event = (avg, 0, 4.0, np.zeros((MAX_TRIES, GRID_SEQ_FEAT_DIM), dtype=np.float32))
        pad = [pad_event] * (LOOK_BACK - len(events))
        window = pad + events
    else:
        window = events[-LOOK_BACK:]

    trials = np.array([w[0] for w in window], np.float32)
    seq = np.stack([trials/7.0, np.full_like(trials, np.std(trials)/7.0)], axis=1)
    seq = seq.reshape(1, LOOK_BACK, 2)

    last = events[-1]
    # diff å·²ç§»é™¤
    wid = np.array([[last[1]]], np.int32) # word_id ç°åœ¨æ˜¯ç´¢å¼• 1
    bias = np.array([[last[2] / 7.0]], np.float32) # user_bias ç°åœ¨æ˜¯ç´¢å¼• 2
    # grid_stat å·²ç§»é™¤
    grid_seq = last[3].reshape(1, MAX_TRIES, GRID_SEQ_FEAT_DIM) # grid_seq ç°åœ¨æ˜¯ç´¢å¼• 3

    p_steps, p_prob = model.predict({
        "input_history": seq,
        # "input_difficulty" å·²ç§»é™¤
        "input_word_id": wid,
        "input_user_bias": bias,
        # "input_grid_stat" å·²ç§»é™¤
        "input_grid_sequence": grid_seq
    }, verbose=0)

    print(f"é¢„æµ‹æ­¥æ•°: {float(np.clip(p_steps, 0, 6.99)):.2f}")
    print(f"æˆåŠŸæ¦‚ç‡: {float(p_prob):.3f}")

# ==========================================================
# å¯åŠ¨å…¥å£
# ==========================================================
if __name__ == "__main__":
    main_train()