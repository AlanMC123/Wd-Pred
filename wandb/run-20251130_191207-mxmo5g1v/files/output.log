===== 加载所有预处理数据集 =====
Step 1: 加载预处理后的数据 - train_data.csv
  读取数据集完成，共3632544条记录，耗时: 2.50秒
  正在拟合新的 Tokenizer...
  创建单词ID完成，词汇表大小: 321，耗时: 17.78秒
  读取单词难度数据...
  读取用户水平数据...
  构建历史数据映射...
  数据加载和预处理完成，总耗时: 29.31秒
  性能统计:
    - 数据读取: 2.50秒
    - 单词ID创建: 17.78秒
    - 单词难度加载: 0.14秒
    - 用户水平加载: 0.15秒
    - 历史映射构建: 7.54秒
Step 1: 加载预处理后的数据 - val_data.csv
  读取数据集完成，共518935条记录，耗时: 0.38秒
  使用已有的 Tokenizer 进行转换...
  创建单词ID完成，词汇表大小: 321，耗时: 1.60秒
  读取单词难度数据...
  读取用户水平数据...
  构建历史数据映射...
  数据加载和预处理完成，总耗时: 8.39秒
  性能统计:
    - 数据读取: 0.38秒
    - 单词ID创建: 1.60秒
    - 单词难度加载: 0.02秒
    - 用户水平加载: 0.13秒
    - 历史映射构建: 6.16秒
Step 1: 加载预处理后的数据 - test_data.csv
  读取数据集完成，共1037870条记录，耗时: 0.86秒
  使用已有的 Tokenizer 进行转换...
  创建单词ID完成，词汇表大小: 321，耗时: 3.39秒
  读取单词难度数据...
  读取用户水平数据...
  构建历史数据映射...
  数据加载和预处理完成，总耗时: 11.53秒
  性能统计:
    - 数据读取: 0.86秒
    - 单词ID创建: 3.39秒
    - 单词难度加载: 0.04秒
    - 用户水平加载: 0.14秒
    - 历史映射构建: 6.84秒
Step 2: 构建四输入样本...
数据集构建完成: 共2468390个样本
Step 2: 构建四输入样本...
数据集构建完成: 共83482个样本
Step 2: 构建四输入样本...
数据集构建完成: 共374266个样本
===== 所有数据集准备完成 =====
数据集划分：训练集 2468390, 验证集 83482, 测试集 374266
✅ 数据加载优化已应用 (cache + prefetch)
Step 3: 构建Transformer神经网络 (Vocab=321, Layers=3)...
WARNING:tensorflow:From D:\Python\Python312\Lib\site-packages\keras\src\backend\tensorflow\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

D:\Python\Python312\Lib\site-packages\keras\src\layers\core\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.
  warnings.warn(
✅ 模型编译完成，使用Focal Loss (gamma=2.0, alpha=0.25)
Step 4: 开始训练 (Epochs=10, Batch=4096, Patience=5)...
提示: 当前使用CPU训练，速度可能较慢。建议使用支持CUDA的GPU以获得显著加速。
Epoch 1/10
[1m 37/603[0m [32m━[0m[37m━━━━━━━━━━━━━━━━━━━[0m [1m22:17[0m 2s/step - loss: 6.4300 - output_steps_loss: 6.3873 - output_success_accuracy: 0.2640 - output_success_loss: 0.0853
